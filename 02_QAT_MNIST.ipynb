{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 02: Writing first MNIST Program\n",
    "By Rahul GAWAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This program is inspired by [GitHub](https://github.com/Hvass-Labs/TensorFlow-Tutorials)\n",
    "\n",
    "Python used: 3.6\n",
    "TensorFlow version: 1.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import urllib.request\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_w = 28\n",
    "img_h = 28\n",
    "img_shape = (img_w, img_h)\n",
    "img_shape_storage = (img_w, img_h, 1)\n",
    "\n",
    "url = \"https://storage.googleapis.com/cvdf-datasets/mnist/\"\n",
    "\n",
    "x_train_file = \"train-images-idx3-ubyte.gz\"\n",
    "y_train_file = \"train-labels-idx1-ubyte.gz\"\n",
    "x_test_file = \"t10k-images-idx3-ubyte.gz\"\n",
    "y_test_file = \"t10k-labels-idx1-ubyte.gz\"\n",
    "\n",
    "target = \"mnist_dataset\"\n",
    "\n",
    "x_train_url = url + x_train_file\n",
    "y_train_url = url + y_train_file\n",
    "x_test_url = url + x_test_file\n",
    "y_test_url = url + y_test_file\n",
    "\n",
    "x_train_rpath = target + \"/\" + x_train_file\n",
    "y_train_rpath = target + \"/\" + y_train_file\n",
    "x_test_rpath = target + \"/\" + x_test_file\n",
    "y_test_rpath = target + \"/\" + y_test_file\n",
    "\n",
    "training_recname = '%s/mnist_%s.tfrecord' % (target, x_train_file)\n",
    "testing_recname = '%s/mnist_%s.tfrecord' % (target, x_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset to a local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print_download_progress(count, block_size, total_size):\n",
    "    \"\"\"\n",
    "    Function used for printing the download progress.\n",
    "    Used as a call-back function in maybe_download_and_extract().\n",
    "    \"\"\"\n",
    "\n",
    "    # Percentage completion.\n",
    "    pct_complete = float(count * block_size) / total_size\n",
    "\n",
    "    # Limit it because rounding errors may cause it to exceed 100%.\n",
    "    pct_complete = min(1.0, pct_complete)\n",
    "\n",
    "    # Status-message. Note the \\r which means the line should overwrite itself.\n",
    "    msg = \"\\r- Download progress: {0:.1%}\".format(pct_complete)\n",
    "\n",
    "    # Print it.\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_img(base_url, filename, download_dir, offset):\n",
    "\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "\n",
    "    file_path = os.path.join(download_dir, filename)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"\\nDownloading\", base_url, \"at\", download_dir, \".\")\n",
    "        file_path, _ = urllib.request.urlretrieve( base_url, file_path, _print_download_progress)\n",
    "\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=offset)\n",
    "\n",
    "    img_size = 28\n",
    "    img_size_flat = img_size * img_size\n",
    "    num_channels = 1\n",
    "    img_shape_full = (img_size, img_size, num_channels)\n",
    "    images_flat = data.reshape(-1, img_size_flat)\n",
    "    return images_flat\n",
    "\n",
    "def download_cls(base_url, filename, download_dir,  offset):\n",
    "\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "\n",
    "    file_path = os.path.join(download_dir, filename)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"\\nDownloading\", base_url, \"at\", download_dir, \".\")\n",
    "        file_path, _ = urllib.request.urlretrieve( base_url, file_path, _print_download_progress)\n",
    "\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8,  offset=offset)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x_train Set 60000\n",
      "y_train_cls Set 60000\n",
      "x_test Set 10000\n",
      "y_test_cls Set 10000\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "\n",
    "#save_path = os.path.join(download_dir, filename)\n",
    "\n",
    "x_train = download_img(x_train_url, x_train_file, target,16)\n",
    "print(\"x_train Set\", len(x_train))\n",
    "\n",
    "y_train_cls = download_cls(y_train_url, y_train_file, target,8)\n",
    "print(\"y_train_cls Set\", len(y_train_cls))\n",
    "\n",
    "x_test = download_img(x_test_url, x_test_file, target,16)\n",
    "print(\"x_test Set\", len(x_test))\n",
    "\n",
    "y_test_cls = download_cls(y_test_url, y_test_file, target,8)\n",
    "print(\"y_test_cls Set\", len(y_test_cls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set 60000  images and  60000  classes.\n",
      "Validation Set 0  images and  0  classes.\n",
      "Test Set 10000  images and  10000  classes.\n"
     ]
    }
   ],
   "source": [
    "num_train = 60000\n",
    "num_val = 5000\n",
    "num_test = 10000\n",
    "\n",
    "X_train = x_train[0:num_train] / 255.0\n",
    "Y_train_cls = y_train_cls[0:num_train]\n",
    "\n",
    "X_val = x_train[num_train:] / 255.0\n",
    "Y_val_cls = y_train_cls[num_train:]\n",
    "\n",
    "print(\"Training Set\", len(X_train), \" images and \", len(Y_train_cls), \" classes.\")\n",
    "print(\"Validation Set\", len(X_val), \" images and \", len(Y_val_cls), \" classes.\")\n",
    "\n",
    "X_test = x_test[0:num_test] / 255.0\n",
    "Y_test_cls = y_test_cls[0:num_test]\n",
    "\n",
    "print(\"Test Set\", len(X_test), \" images and \", len(Y_test_cls), \" classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None, img_w=28, img_h=28):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "\n",
    "\n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the first images from the test-set.\n",
    "images = X_test[0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = Y_test_cls[0:9]\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images=images, cls_true=cls_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_cls_char = Y_train_cls.astype(np.int)\n",
    "y_val_cls_char = Y_val_cls.astype(np.int)\n",
    "y_test_cls_char = Y_test_cls.astype(np.int)\n",
    "\n",
    "Y_train_OHE = np.eye(10, dtype=float)[y_train_cls_char]\n",
    "Y_val_OHE = np.eye(10, dtype=float)[y_val_cls_char]\n",
    "Y_test_OHE = np.eye(10, dtype=float)[y_test_cls_char]\n",
    "\n",
    "Y_test_OHE[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-record (Optional)\n",
    "\n",
    "[Referred this implementation to create tf.record](https://github.com/tensorflow/models/blob/master/research/slim/datasets/download_and_convert_mnist.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int64_feature(values):\n",
    "  if not isinstance(values, (tuple, list)):\n",
    "    values = [values]\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tfexample(image_data, image_format, height, width, class_id):\n",
    "  return tf.train.Example(features=tf.train.Features(feature={\n",
    "      'image/encoded':  tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_data])),\n",
    "      'image/format':  tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_format])),\n",
    "      'image/class/label': int64_feature(class_id),\n",
    "      'image/height': int64_feature(height),\n",
    "      'image/width': int64_feature(width)\n",
    "  }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _add_to_tfrecord(data_filename, labels_filename, num_images, tfrecord_writer, images, labels):\n",
    "    with tf.Graph().as_default():\n",
    "        image = tf.placeholder(dtype=tf.uint8, shape=img_shape_storage)\n",
    "        encoded_png = tf.image.encode_png(image)\n",
    "\n",
    "        num_images = len(images)\n",
    "        images = images.reshape(-1, img_w, img_h, 1)\n",
    "\n",
    "        with tf.Session('') as sess:\n",
    "            for j in range(num_images):\n",
    "                sys.stdout.write('\\r>> Converting image %d/%d' % (j + 1, num_images))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                png_string = sess.run(encoded_png, feed_dict={image: images[j]})\n",
    "                example = image_to_tfexample(png_string, 'png'.encode(), img_w, img_h, labels[j])\n",
    "                tfrecord_writer.write(example.SerializeToString())\n",
    "\n",
    "#                pngFname = target + \"/\" + \"train_%d.png\" % (j)\n",
    "#                pngFile=open(pngFname,'wb')\n",
    "#                pngFile.write(png_string);\n",
    "#                pngFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train Set 60000\n",
      "y_train_cls Set 60000\n",
      "x_test Set 10000\n",
      "y_test_cls Set 10000\n",
      "60000\n",
      ">> Converting image 3000/60000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Converting image 8563/60000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Converting image 13895/60000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Converting image 19392/60000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Converting image 24661/60000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Converting image 29933/60000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Converting image 35350/60000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Converting image 40826/60000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Converting image 46211/60000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Converting image 49361/60000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Converting image 51427/60000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Converting image 54742/60000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Converting image 56767/60000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Converting image 89/10000000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Converting image 2259/10000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Converting image 5703/10000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Converting image 8014/10000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Converting image 10000/10000"
     ]
    }
   ],
   "source": [
    "x_train = download_img(x_train_url, x_train_file, target,16)\n",
    "print(\"x_train Set\", len(x_train))\n",
    "\n",
    "y_train_cls = download_cls(y_train_url, y_train_file, target,8)\n",
    "print(\"y_train_cls Set\", len(y_train_cls))\n",
    "\n",
    "x_test = download_img(x_test_url, x_test_file, target,16)\n",
    "print(\"x_test Set\", len(x_test))\n",
    "\n",
    "y_test_cls = download_cls(y_test_url, y_test_file, target,8)\n",
    "print(\"y_test_cls Set\", len(y_test_cls))\n",
    "\n",
    "print(len(x_train))\n",
    "\n",
    "if not os.path.exists(training_recname):\n",
    "    with tf.python_io.TFRecordWriter(training_recname) as tfrecord_writer:\n",
    "        _add_to_tfrecord(x_train_rpath, y_train_rpath, 60000, tfrecord_writer, x_train, y_train_cls)\n",
    "\n",
    "if not os.path.exists(testing_recname):\n",
    "    with tf.python_io.TFRecordWriter(testing_recname) as tfrecord_writer:\n",
    "        _add_to_tfrecord(x_test_rpath, y_test_rpath, 10000, tfrecord_writer, x_test, y_test_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(record):\n",
    "    keys_to_features = {\n",
    "        \"image/encoded\":  tf.FixedLenFeature([], tf.string),\n",
    "        \"image/format\":  tf.FixedLenFeature([], tf.string),\n",
    "        \"image/class/label\": tf.FixedLenFeature([], tf.int64),\n",
    "        \"image/height\": tf.FixedLenFeature([], tf.int64),\n",
    "        \"image/width\": tf.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "\n",
    "    parsed = tf.parse_single_example(record, keys_to_features)\n",
    "\n",
    "    height = tf.cast(parsed[\"image/height\"], tf.int32)\n",
    "    width = tf.cast(parsed[\"image/width\"], tf.int32)\n",
    "    label = tf.cast(parsed[\"image/class/label\"], tf.int32)\n",
    "    image = tf.cast(tf.image.decode_png(parsed[\"image/encoded\"], channels=1), tf.float32)\n",
    "    \n",
    " \n",
    "    return {'image': image}, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(filenames):\n",
    "    \n",
    "    BATCH_SIZE = 128\n",
    "    THREADS = 4\n",
    "    PREFETCH = 64\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(filenames=filenames, num_parallel_reads=THREADS)\n",
    "    dataset = dataset.apply(\n",
    "        tf.contrib.data.shuffle_and_repeat(1024, 1)\n",
    "    )\n",
    "    dataset = dataset.apply(\n",
    "        tf.contrib.data.map_and_batch(parser, BATCH_SIZE)\n",
    "    )\n",
    "\n",
    "    dataset = dataset.prefetch(buffer_size=PREFETCH)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    return input_fn(filenames=[training_recname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_input_fn():\n",
    "    return input_fn(filenames=[testing_recname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode, params):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer\n",
    "    # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "    # MNIST images are 28x28 pixels, and have one color channel\n",
    "\n",
    "    net = features[\"image\"]\n",
    "\n",
    "    net = tf.identity(net, name=\"input_tensor\")\n",
    "\n",
    "    net = tf.reshape(net, [-1, 28, 28, 1])\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "    # Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    conv1 = tf.layers.conv2d(inputs=net, filters=32,kernel_size=[5, 5],padding=\"same\", activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    # First max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Convolutional Layer #2\n",
    "    # Computes 64 features using a 5x5 filter.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    conv2 = tf.layers.conv2d(inputs=pool1,filters=64,kernel_size=[5, 5],padding=\"same\",activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #2\n",
    "    # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Flatten tensor into a batch of vectors\n",
    "    # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "    # Dense Layer\n",
    "    # Densely connected layer with 1024 neurons\n",
    "    # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    # Output Tensor Shape: [batch_size, 1024]\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "    # Add dropout operation; 0.6 probability that element will be kept\n",
    "    dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits layer\n",
    "    # Input Tensor Shape: [batch_size, 1024]\n",
    "    # Output Tensor Shape: [batch_size, 10]\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1), \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(loss=loss,global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])\n",
    "    }\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '02_QAT_MNIST', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f99f4c948d0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 34.141724, step = 0\n",
      "INFO:tensorflow:global_step/sec: 275.066\n",
      "INFO:tensorflow:loss = 0.5594313, step = 100 (0.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 349.13\n",
      "INFO:tensorflow:loss = 0.40125906, step = 200 (0.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.092\n",
      "INFO:tensorflow:loss = 0.30469844, step = 300 (0.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 341.872\n",
      "INFO:tensorflow:loss = 0.33866203, step = 400 (0.293 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 469 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.096779294.\n",
      "Time Take per Iternation of training is : 186.632790%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:44:15\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-469\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:44:15\n",
      "INFO:tensorflow:Saving dict for global step 469: accuracy = 0.9568, global_step = 469, loss = 0.14272115\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 469: 02_QAT_MNIST/model.ckpt-469\n",
      "Classification accuracy: 95.68%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-469\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 469 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.21728507, step = 469\n",
      "INFO:tensorflow:global_step/sec: 294.172\n",
      "INFO:tensorflow:loss = 0.23039213, step = 569 (0.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 342.539\n",
      "INFO:tensorflow:loss = 0.25056535, step = 669 (0.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 348.923\n",
      "INFO:tensorflow:loss = 0.13553962, step = 769 (0.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 349.557\n",
      "INFO:tensorflow:loss = 0.10146936, step = 869 (0.286 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 938 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.20349854.\n",
      "Time Take per Iternation of training is : 182.871022%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:44:17\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-938\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:44:18\n",
      "INFO:tensorflow:Saving dict for global step 938: accuracy = 0.9684, global_step = 938, loss = 0.099832945\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 938: 02_QAT_MNIST/model.ckpt-938\n",
      "Classification accuracy: 96.84%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-938\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 938 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0925333, step = 938\n",
      "INFO:tensorflow:global_step/sec: 292.998\n",
      "INFO:tensorflow:loss = 0.0902694, step = 1038 (0.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 348.404\n",
      "INFO:tensorflow:loss = 0.07896013, step = 1138 (0.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 341.801\n",
      "INFO:tensorflow:loss = 0.1839309, step = 1238 (0.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 345.893\n",
      "INFO:tensorflow:loss = 0.3256532, step = 1338 (0.289 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1407 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.1026532.\n",
      "Time Take per Iternation of training is : 190.271871%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:44:20\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-1407\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:44:20\n",
      "INFO:tensorflow:Saving dict for global step 1407: accuracy = 0.9743, global_step = 1407, loss = 0.07897136\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1407: 02_QAT_MNIST/model.ckpt-1407\n",
      "Classification accuracy: 97.43%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-1407\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1407 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.24318813, step = 1407\n",
      "INFO:tensorflow:global_step/sec: 297.558\n",
      "INFO:tensorflow:loss = 0.25123882, step = 1507 (0.336 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.305\n",
      "INFO:tensorflow:loss = 0.10090473, step = 1607 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 347.021\n",
      "INFO:tensorflow:loss = 0.053008478, step = 1707 (0.288 sec)\n",
      "INFO:tensorflow:global_step/sec: 345.482\n",
      "INFO:tensorflow:loss = 0.110683784, step = 1807 (0.289 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1876 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.05776352.\n",
      "Time Take per Iternation of training is : 182.154752%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:44:22\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-1876\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:44:22\n",
      "INFO:tensorflow:Saving dict for global step 1876: accuracy = 0.9761, global_step = 1876, loss = 0.069930635\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1876: 02_QAT_MNIST/model.ckpt-1876\n",
      "Classification accuracy: 97.61%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-1876\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1876 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.22857036, step = 1876\n",
      "INFO:tensorflow:global_step/sec: 295.253\n",
      "INFO:tensorflow:loss = 0.06598028, step = 1976 (0.339 sec)\n",
      "INFO:tensorflow:global_step/sec: 347.028\n",
      "INFO:tensorflow:loss = 0.11582986, step = 2076 (0.288 sec)\n",
      "INFO:tensorflow:global_step/sec: 348.629\n",
      "INFO:tensorflow:loss = 0.1455675, step = 2176 (0.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 341.436\n",
      "INFO:tensorflow:loss = 0.1223211, step = 2276 (0.293 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2345 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.051787317.\n",
      "Time Take per Iternation of training is : 185.826071%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:44:24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-2345\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:44:24\n",
      "INFO:tensorflow:Saving dict for global step 2345: accuracy = 0.9788, global_step = 2345, loss = 0.062345862\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2345: 02_QAT_MNIST/model.ckpt-2345\n",
      "Classification accuracy: 97.88%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-2345\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2345 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.13359956, step = 2345\n",
      "INFO:tensorflow:global_step/sec: 296.121\n",
      "INFO:tensorflow:loss = 0.18453097, step = 2445 (0.338 sec)\n",
      "INFO:tensorflow:global_step/sec: 348.648\n",
      "INFO:tensorflow:loss = 0.2162729, step = 2545 (0.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 345.955\n",
      "INFO:tensorflow:loss = 0.10601491, step = 2645 (0.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 341.556\n",
      "INFO:tensorflow:loss = 0.09566247, step = 2745 (0.293 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2814 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.29486477.\n",
      "Time Take per Iternation of training is : 191.514503%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:44:26\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-2814\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:44:27\n",
      "INFO:tensorflow:Saving dict for global step 2814: accuracy = 0.9824, global_step = 2814, loss = 0.05530726\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2814: 02_QAT_MNIST/model.ckpt-2814\n",
      "Classification accuracy: 98.24%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-2814\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2814 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.095627315, step = 2814\n",
      "INFO:tensorflow:global_step/sec: 292.982\n",
      "INFO:tensorflow:loss = 0.0829998, step = 2914 (0.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 351.584\n",
      "INFO:tensorflow:loss = 0.14582026, step = 3014 (0.284 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.178\n",
      "INFO:tensorflow:loss = 0.056392394, step = 3114 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 336.143\n",
      "INFO:tensorflow:loss = 0.12132141, step = 3214 (0.297 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3283 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.031060718.\n",
      "Time Take per Iternation of training is : 186.409284%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:44:28\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-3283\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:44:29\n",
      "INFO:tensorflow:Saving dict for global step 3283: accuracy = 0.9811, global_step = 3283, loss = 0.05359058\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3283: 02_QAT_MNIST/model.ckpt-3283\n",
      "Classification accuracy: 98.11%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-3283\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3283 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.07262947, step = 3283\n",
      "INFO:tensorflow:global_step/sec: 287.867\n",
      "INFO:tensorflow:loss = 0.041557945, step = 3383 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 342.909\n",
      "INFO:tensorflow:loss = 0.07698646, step = 3483 (0.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 337.345\n",
      "INFO:tensorflow:loss = 0.059827976, step = 3583 (0.296 sec)\n",
      "INFO:tensorflow:global_step/sec: 336.065\n",
      "INFO:tensorflow:loss = 0.079701215, step = 3683 (0.298 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3752 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.01684947.\n",
      "Time Take per Iternation of training is : 187.213698%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:44:31\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-3752\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:44:31\n",
      "INFO:tensorflow:Saving dict for global step 3752: accuracy = 0.9826, global_step = 3752, loss = 0.05026841\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3752: 02_QAT_MNIST/model.ckpt-3752\n",
      "Classification accuracy: 98.26%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-3752\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3752 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0551815, step = 3752\n",
      "INFO:tensorflow:global_step/sec: 296.12\n",
      "INFO:tensorflow:loss = 0.0632747, step = 3852 (0.338 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.052\n",
      "INFO:tensorflow:loss = 0.06311327, step = 3952 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 338.049\n",
      "INFO:tensorflow:loss = 0.07146879, step = 4052 (0.296 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.321\n",
      "INFO:tensorflow:loss = 0.10253192, step = 4152 (0.290 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4221 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.10019266.\n",
      "Time Take per Iternation of training is : 189.361262%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:44:33\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-4221\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:44:33\n",
      "INFO:tensorflow:Saving dict for global step 4221: accuracy = 0.9851, global_step = 4221, loss = 0.04673959\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4221: 02_QAT_MNIST/model.ckpt-4221\n",
      "Classification accuracy: 98.51%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-4221\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 4221 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.050390773, step = 4221\n",
      "INFO:tensorflow:global_step/sec: 293.36\n",
      "INFO:tensorflow:loss = 0.06442755, step = 4321 (0.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 339.557\n",
      "INFO:tensorflow:loss = 0.097380355, step = 4421 (0.294 sec)\n",
      "INFO:tensorflow:global_step/sec: 348.336\n",
      "INFO:tensorflow:loss = 0.12249404, step = 4521 (0.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 341.055\n",
      "INFO:tensorflow:loss = 0.057013292, step = 4621 (0.293 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4690 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0110018505.\n",
      "Time Take per Iternation of training is : 184.932016%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:44:35\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-4690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:44:36\n",
      "INFO:tensorflow:Saving dict for global step 4690: accuracy = 0.9847, global_step = 4690, loss = 0.044876948\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4690: 02_QAT_MNIST/model.ckpt-4690\n",
      "Classification accuracy: 98.47%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-4690\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 4690 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.08276269, step = 4690\n",
      "INFO:tensorflow:global_step/sec: 288.662\n",
      "INFO:tensorflow:loss = 0.037841134, step = 4790 (0.347 sec)\n",
      "INFO:tensorflow:global_step/sec: 340.917\n",
      "INFO:tensorflow:loss = 0.012787848, step = 4890 (0.293 sec)\n",
      "INFO:tensorflow:global_step/sec: 345.201\n",
      "INFO:tensorflow:loss = 0.059679523, step = 4990 (0.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 339.856\n",
      "INFO:tensorflow:loss = 0.0709303, step = 5090 (0.294 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5159 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.052576657.\n",
      "Time Take per Iternation of training is : 183.824240%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:44:37\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-5159\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:44:38\n",
      "INFO:tensorflow:Saving dict for global step 5159: accuracy = 0.9855, global_step = 5159, loss = 0.046196017\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5159: 02_QAT_MNIST/model.ckpt-5159\n",
      "Classification accuracy: 98.55%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-5159\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 5159 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.066521004, step = 5159\n",
      "INFO:tensorflow:global_step/sec: 289.037\n",
      "INFO:tensorflow:loss = 0.08702372, step = 5259 (0.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 335.123\n",
      "INFO:tensorflow:loss = 0.07709986, step = 5359 (0.299 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.889\n",
      "INFO:tensorflow:loss = 0.075306244, step = 5459 (0.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 323.289\n",
      "INFO:tensorflow:loss = 0.074380204, step = 5559 (0.309 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5628 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.027237207.\n",
      "Time Take per Iternation of training is : 193.181183%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:44:40\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-5628\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:44:40\n",
      "INFO:tensorflow:Saving dict for global step 5628: accuracy = 0.9859, global_step = 5628, loss = 0.041282535\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5628: 02_QAT_MNIST/model.ckpt-5628\n",
      "Classification accuracy: 98.59%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-5628\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 5628 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.06748679, step = 5628\n",
      "INFO:tensorflow:global_step/sec: 288.319\n",
      "INFO:tensorflow:loss = 0.027188458, step = 5728 (0.347 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.663\n",
      "INFO:tensorflow:loss = 0.12818146, step = 5828 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 345.037\n",
      "INFO:tensorflow:loss = 0.08466127, step = 5928 (0.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 342.229\n",
      "INFO:tensorflow:loss = 0.042312194, step = 6028 (0.292 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6097 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.012948032.\n",
      "Time Take per Iternation of training is : 185.617949%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:44:42\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-6097\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:44:42\n",
      "INFO:tensorflow:Saving dict for global step 6097: accuracy = 0.9859, global_step = 6097, loss = 0.0398976\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6097: 02_QAT_MNIST/model.ckpt-6097\n",
      "Classification accuracy: 98.59%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-6097\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 6097 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.04575468, step = 6097\n",
      "INFO:tensorflow:global_step/sec: 295.785\n",
      "INFO:tensorflow:loss = 0.028441228, step = 6197 (0.339 sec)\n",
      "INFO:tensorflow:global_step/sec: 341.72\n",
      "INFO:tensorflow:loss = 0.100134805, step = 6297 (0.293 sec)\n",
      "INFO:tensorflow:global_step/sec: 346.475\n",
      "INFO:tensorflow:loss = 0.11873847, step = 6397 (0.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 337.221\n",
      "INFO:tensorflow:loss = 0.04869096, step = 6497 (0.296 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6566 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0052127056.\n",
      "Time Take per Iternation of training is : 189.784634%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:44:44\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-6566\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:44:44\n",
      "INFO:tensorflow:Saving dict for global step 6566: accuracy = 0.9875, global_step = 6566, loss = 0.038622636\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6566: 02_QAT_MNIST/model.ckpt-6566\n",
      "Classification accuracy: 98.75%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-6566\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 6566 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.038662013, step = 6566\n",
      "INFO:tensorflow:global_step/sec: 287.518\n",
      "INFO:tensorflow:loss = 0.037199877, step = 6666 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 346.785\n",
      "INFO:tensorflow:loss = 0.020014262, step = 6766 (0.288 sec)\n",
      "INFO:tensorflow:global_step/sec: 341.986\n",
      "INFO:tensorflow:loss = 0.058892373, step = 6866 (0.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 336.575\n",
      "INFO:tensorflow:loss = 0.02205208, step = 6966 (0.297 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7035 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.015111814.\n",
      "Time Take per Iternation of training is : 189.254033%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:44:46\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-7035\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:44:47\n",
      "INFO:tensorflow:Saving dict for global step 7035: accuracy = 0.9872, global_step = 7035, loss = 0.038667336\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 7035: 02_QAT_MNIST/model.ckpt-7035\n",
      "Classification accuracy: 98.72%"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "model = tf.estimator.Estimator(model_fn=cnn_model_fn,\n",
    "                               params={\"learning_rate\": 1e-4},\n",
    "                               model_dir=\"02_QAT_MNIST\")\n",
    "\n",
    "import timeit\n",
    "\n",
    "EPOCHS = 15\n",
    "STEP_SIZE = 500\n",
    "count = 0\n",
    "\n",
    "while (count < EPOCHS):\n",
    "    start_time = timeit.default_timer()\n",
    "    model.train(input_fn=train_input_fn, steps=STEP_SIZE)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    sys.stdout.write(\"Time Take per Iternation of training is : {0:%}\".format(elapsed))\n",
    "\n",
    "    result = model.evaluate(input_fn=val_input_fn)\n",
    "    #print(result)\n",
    "    sys.stdout.write(\"Classification accuracy: {0:.2%}\".format(result[\"accuracy\"]))\n",
    "    sys.stdout.flush()\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-7035\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./02_QAT_MNIST/export/temp-b'1546904687'/saved_model.pbtxt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'./02_QAT_MNIST/export/1546904687'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_serving_input_receiver_fn():\n",
    "    inputs = {'image': tf.placeholder(shape=[28,28,1], dtype=tf.float32, name='image')}\n",
    "    return tf.estimator.export.build_raw_serving_input_receiver_fn(inputs)\n",
    "\n",
    "export_dir = os.path.join('./02_QAT_MNIST/', 'export')\n",
    "\n",
    "if tf.gfile.Exists(export_dir):\n",
    "        tf.gfile.DeleteRecursively(export_dir)\n",
    "\n",
    "model.export_savedmodel(\n",
    "    export_dir_base=export_dir,\n",
    "    serving_input_receiver_fn=make_serving_input_receiver_fn(),\n",
    "    as_text=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./02_QAT_MNIST/export/1546904687/variables/variables\n"
     ]
    }
   ],
   "source": [
    "export_dir = os.path.join('./02_QAT_MNIST/', 'export')\n",
    "saved_model_dir = os.path.join(export_dir, os.listdir(export_dir)[-1]) \n",
    "\n",
    "predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "  export_dir = saved_model_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Result: Pred. No. : 0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy\n",
    "import random\n",
    "\n",
    "dir = target + \"/PNGs/\"\n",
    "pngFname = dir + random.choice(os.listdir(dir))\n",
    "\n",
    "pngFile=open(pngFname,'rb')\n",
    "png_string = pngFile.read();\n",
    "pngFile.close()\n",
    "\n",
    "#sample = tf.cast(tf.image.decode_png(png_string, channels=1), tf.int8)\n",
    "sample = tf.cast(tf.image.decode_png(png_string, channels=1), tf.float32)\n",
    "\n",
    "a = np.ones(shape=(28,28,1),dtype=np.float32)    \n",
    "\n",
    "img = tf.reshape(sample, [28, 28, 1])\n",
    "\n",
    "with sess.as_default():\n",
    "    a = img.eval()\n",
    "\n",
    "output = predictor_fn({'image': a})\n",
    "#print(output)\n",
    "#print (\"train_data.shape: \" + str(sample.get_shape()))\n",
    "#print (\"train_data.shape: \" + str(img.get_shape()))\n",
    "\n",
    "max = 0\n",
    "\n",
    "for x in range(10):\n",
    "    if output['probabilities'][0][x] >= output['probabilities'][0][max]:\n",
    "        max = x\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.imshow(a.reshape(img_shape), cmap='binary')\n",
    "xlabel = \"Result: Pred. No. : {0}\".format(max)\n",
    "ax.set_title(xlabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
