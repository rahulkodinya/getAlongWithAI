{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 02: Writing first MNIST Program\n",
    "By Rahul GAWAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This program is inspired by [GitHub](https://github.com/Hvass-Labs/TensorFlow-Tutorials)\n",
    "\n",
    "Python used: 3.6\n",
    "TensorFlow version: 1.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import urllib.request\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_w = 28\n",
    "img_h = 28\n",
    "img_shape = (img_w, img_h)\n",
    "img_shape_storage = (img_w, img_h, 1)\n",
    "\n",
    "url = \"https://storage.googleapis.com/cvdf-datasets/mnist/\"\n",
    "\n",
    "x_train_file = \"train-images-idx3-ubyte.gz\"\n",
    "y_train_file = \"train-labels-idx1-ubyte.gz\"\n",
    "x_test_file = \"t10k-images-idx3-ubyte.gz\"\n",
    "y_test_file = \"t10k-labels-idx1-ubyte.gz\"\n",
    "\n",
    "target = \"mnist_dataset\"\n",
    "\n",
    "x_train_url = url + x_train_file\n",
    "y_train_url = url + y_train_file\n",
    "x_test_url = url + x_test_file\n",
    "y_test_url = url + y_test_file\n",
    "\n",
    "x_train_rpath = target + \"/\" + x_train_file\n",
    "y_train_rpath = target + \"/\" + y_train_file\n",
    "x_test_rpath = target + \"/\" + x_test_file\n",
    "y_test_rpath = target + \"/\" + y_test_file\n",
    "\n",
    "training_recname = '%s/mnist_%s.tfrecord' % (target, x_train_file)\n",
    "testing_recname = '%s/mnist_%s.tfrecord' % (target, x_test_file)\n",
    "\n",
    "export_dir = os.path.join('./02_QAT_MNIST/', 'export')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset to a local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print_download_progress(count, block_size, total_size):\n",
    "    \"\"\"\n",
    "    Function used for printing the download progress.\n",
    "    Used as a call-back function in maybe_download_and_extract().\n",
    "    \"\"\"\n",
    "\n",
    "    # Percentage completion.\n",
    "    pct_complete = float(count * block_size) / total_size\n",
    "\n",
    "    # Limit it because rounding errors may cause it to exceed 100%.\n",
    "    pct_complete = min(1.0, pct_complete)\n",
    "\n",
    "    # Status-message. Note the \\r which means the line should overwrite itself.\n",
    "    msg = \"\\r- Download progress: {0:.1%}\".format(pct_complete)\n",
    "\n",
    "    # Print it.\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_img(base_url, filename, download_dir, offset):\n",
    "\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "\n",
    "    file_path = os.path.join(download_dir, filename)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"\\nDownloading\", base_url, \"at\", download_dir, \".\")\n",
    "        file_path, _ = urllib.request.urlretrieve( base_url, file_path, _print_download_progress)\n",
    "\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=offset)\n",
    "\n",
    "    img_size = 28\n",
    "    img_size_flat = img_size * img_size\n",
    "    num_channels = 1\n",
    "    img_shape_full = (img_size, img_size, num_channels)\n",
    "    images_flat = data.reshape(-1, img_size_flat)\n",
    "    return images_flat\n",
    "\n",
    "def download_cls(base_url, filename, download_dir,  offset):\n",
    "\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "\n",
    "    file_path = os.path.join(download_dir, filename)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"\\nDownloading\", base_url, \"at\", download_dir, \".\")\n",
    "        file_path, _ = urllib.request.urlretrieve( base_url, file_path, _print_download_progress)\n",
    "\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8,  offset=offset)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x_train Set 60000\n",
      "y_train_cls Set 60000\n",
      "x_test Set 10000\n",
      "y_test_cls Set 10000\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "\n",
    "#save_path = os.path.join(download_dir, filename)\n",
    "\n",
    "x_train = download_img(x_train_url, x_train_file, target,16)\n",
    "print(\"x_train Set\", len(x_train))\n",
    "\n",
    "y_train_cls = download_cls(y_train_url, y_train_file, target,8)\n",
    "print(\"y_train_cls Set\", len(y_train_cls))\n",
    "\n",
    "x_test = download_img(x_test_url, x_test_file, target,16)\n",
    "print(\"x_test Set\", len(x_test))\n",
    "\n",
    "y_test_cls = download_cls(y_test_url, y_test_file, target,8)\n",
    "print(\"y_test_cls Set\", len(y_test_cls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set 60000  images and  60000  classes.\n",
      "Validation Set 0  images and  0  classes.\n",
      "Test Set 10000  images and  10000  classes.\n"
     ]
    }
   ],
   "source": [
    "num_train = 60000\n",
    "num_val = 5000\n",
    "num_test = 10000\n",
    "\n",
    "X_train = x_train[0:num_train] / 255.0\n",
    "Y_train_cls = y_train_cls[0:num_train]\n",
    "\n",
    "X_val = x_train[num_train:] / 255.0\n",
    "Y_val_cls = y_train_cls[num_train:]\n",
    "\n",
    "print(\"Training Set\", len(X_train), \" images and \", len(Y_train_cls), \" classes.\")\n",
    "print(\"Validation Set\", len(X_val), \" images and \", len(Y_val_cls), \" classes.\")\n",
    "\n",
    "X_test = x_test[0:num_test] / 255.0\n",
    "Y_test_cls = y_test_cls[0:num_test]\n",
    "\n",
    "print(\"Test Set\", len(X_test), \" images and \", len(Y_test_cls), \" classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None, img_w=28, img_h=28):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "\n",
    "\n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the first images from the test-set.\n",
    "images = X_test[0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = Y_test_cls[0:9]\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images=images, cls_true=cls_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_cls_char = Y_train_cls.astype(np.int)\n",
    "y_val_cls_char = Y_val_cls.astype(np.int)\n",
    "y_test_cls_char = Y_test_cls.astype(np.int)\n",
    "\n",
    "Y_train_OHE = np.eye(10, dtype=float)[y_train_cls_char]\n",
    "Y_val_OHE = np.eye(10, dtype=float)[y_val_cls_char]\n",
    "Y_test_OHE = np.eye(10, dtype=float)[y_test_cls_char]\n",
    "\n",
    "Y_test_OHE[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-record (Optional)\n",
    "\n",
    "[Referred this implementation to create tf.record](https://github.com/tensorflow/models/blob/master/research/slim/datasets/download_and_convert_mnist.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int64_feature(values):\n",
    "  if not isinstance(values, (tuple, list)):\n",
    "    values = [values]\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tfexample(image_data, image_format, height, width, class_id):\n",
    "  return tf.train.Example(features=tf.train.Features(feature={\n",
    "      'image/encoded':  tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_data])),\n",
    "      'image/format':  tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_format])),\n",
    "      'image/class/label': int64_feature(class_id),\n",
    "      'image/height': int64_feature(height),\n",
    "      'image/width': int64_feature(width)\n",
    "  }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _add_to_tfrecord(data_filename, labels_filename, num_images, tfrecord_writer, images, labels):\n",
    "    with tf.Graph().as_default():\n",
    "        image = tf.placeholder(dtype=tf.uint8, shape=img_shape_storage)\n",
    "        encoded_png = tf.image.encode_png(image)\n",
    "\n",
    "        num_images = len(images)\n",
    "        images = images.reshape(-1, img_w, img_h, 1)\n",
    "\n",
    "        with tf.Session('') as sess:\n",
    "            for j in range(num_images):\n",
    "                sys.stdout.write('\\r>> Converting image %d/%d' % (j + 1, num_images))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                png_string = sess.run(encoded_png, feed_dict={image: images[j]})\n",
    "                example = image_to_tfexample(png_string, 'png'.encode(), img_w, img_h, labels[j])\n",
    "                tfrecord_writer.write(example.SerializeToString())\n",
    "\n",
    "#                pngFname = target + \"/\" + \"train_%d.png\" % (j)\n",
    "#                pngFile=open(pngFname,'wb')\n",
    "#                pngFile.write(png_string);\n",
    "#                pngFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train Set 60000\n",
      "y_train_cls Set 60000\n",
      "x_test Set 10000\n",
      "y_test_cls Set 10000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "x_train = download_img(x_train_url, x_train_file, target,16)\n",
    "print(\"x_train Set\", len(x_train))\n",
    "\n",
    "y_train_cls = download_cls(y_train_url, y_train_file, target,8)\n",
    "print(\"y_train_cls Set\", len(y_train_cls))\n",
    "\n",
    "x_test = download_img(x_test_url, x_test_file, target,16)\n",
    "print(\"x_test Set\", len(x_test))\n",
    "\n",
    "y_test_cls = download_cls(y_test_url, y_test_file, target,8)\n",
    "print(\"y_test_cls Set\", len(y_test_cls))\n",
    "\n",
    "print(len(x_train))\n",
    "\n",
    "if not os.path.exists(training_recname):\n",
    "    with tf.python_io.TFRecordWriter(training_recname) as tfrecord_writer:\n",
    "        _add_to_tfrecord(x_train_rpath, y_train_rpath, 60000, tfrecord_writer, x_train, y_train_cls)\n",
    "\n",
    "if not os.path.exists(testing_recname):\n",
    "    with tf.python_io.TFRecordWriter(testing_recname) as tfrecord_writer:\n",
    "        _add_to_tfrecord(x_test_rpath, y_test_rpath, 10000, tfrecord_writer, x_test, y_test_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(record):\n",
    "    keys_to_features = {\n",
    "        \"image/encoded\":  tf.FixedLenFeature([], tf.string),\n",
    "        \"image/format\":  tf.FixedLenFeature([], tf.string),\n",
    "        \"image/class/label\": tf.FixedLenFeature([], tf.int64),\n",
    "        \"image/height\": tf.FixedLenFeature([], tf.int64),\n",
    "        \"image/width\": tf.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "\n",
    "    parsed = tf.parse_single_example(record, keys_to_features)\n",
    "\n",
    "    height = tf.cast(parsed[\"image/height\"], tf.int32)\n",
    "    width = tf.cast(parsed[\"image/width\"], tf.int32)\n",
    "    label = tf.cast(parsed[\"image/class/label\"], tf.int32)\n",
    "    image = tf.cast(tf.image.decode_png(parsed[\"image/encoded\"], channels=1), tf.float32)\n",
    "    \n",
    " \n",
    "    return {'image': image}, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(filenames):\n",
    "    \n",
    "    BATCH_SIZE = 128\n",
    "    THREADS = 4\n",
    "    PREFETCH = 64\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(filenames=filenames, num_parallel_reads=THREADS)\n",
    "    dataset = dataset.apply(\n",
    "        tf.contrib.data.shuffle_and_repeat(1024, 1)\n",
    "    )\n",
    "    dataset = dataset.apply(\n",
    "        tf.contrib.data.map_and_batch(parser, BATCH_SIZE)\n",
    "    )\n",
    "\n",
    "    dataset = dataset.prefetch(buffer_size=PREFETCH)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    return input_fn(filenames=[training_recname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_input_fn():\n",
    "    return input_fn(filenames=[testing_recname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode, params):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer\n",
    "    # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "    # MNIST images are 28x28 pixels, and have one color channel\n",
    "\n",
    "    # Print all of the operations in the default graph.\n",
    "\n",
    "    net = features[\"image\"]\n",
    "\n",
    "    net = tf.identity(net, name=\"input_tensor\")\n",
    "\n",
    "    net = tf.reshape(net, [-1, 28, 28, 1])\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "    # Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    conv1 = tf.layers.conv2d(inputs=net, filters=32,kernel_size=[5, 5],padding=\"same\", activation=tf.nn.relu, name=\"conv1-layer\")\n",
    "    \n",
    "    # Pooling Layer #1\n",
    "    # First max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2, name=\"pool1-layer\")\n",
    "\n",
    "    # Convolutional Layer #2\n",
    "    # Computes 64 features using a 5x5 filter.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    conv2 = tf.layers.conv2d(inputs=pool1,filters=64,kernel_size=[5, 5],padding=\"same\",activation=tf.nn.relu, name=\"conv2-layer\")\n",
    "\n",
    "    # Pooling Layer #2\n",
    "    # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2, name=\"pool2-layer\")\n",
    "\n",
    "    # Flatten tensor into a batch of vectors\n",
    "    # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "    # Dense Layer\n",
    "    # Densely connected layer with 1024 neurons\n",
    "    # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    # Output Tensor Shape: [batch_size, 1024]\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu, name=\"dense-layer\")\n",
    "\n",
    "    # Add dropout operation; 0.6 probability that element will be kept\n",
    "    dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits layer\n",
    "    # Input Tensor Shape: [batch_size, 1024]\n",
    "    # Output Tensor Shape: [batch_size, 10]\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1), \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "#     if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "#         # Call rewriter to produce graph with fake quant ops and folded batch norms\n",
    "#         # quant_delay delays start of quantization till quant_delay steps, allowing\n",
    "#         # for better model accuracy.\n",
    "#         g = tf.get_default_graph()\n",
    "#         tf.contrib.quantize.create_training_graph(input_graph=g, quant_delay=1000)\n",
    "\n",
    "#     if mode == tf.estimator.ModeKeys.EVAL:\n",
    "#         # Call the eval rewrite which rewrites the graph in-place with\n",
    "#         # FakeQuantization nodes and fold batchnorm for eval.\n",
    "#         g = tf.get_default_graph()\n",
    "#         tf.contrib.quantize.create_eval_graph(input_graph=g)     \n",
    "\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(loss=loss,global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])\n",
    "    }\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '02_QAT_MNIST', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f39b930b780>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 44.585316, step = 0\n",
      "INFO:tensorflow:Saving checkpoints for 100 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.6103306.\n",
      "Time Take per Iternation of training is : 87.522112%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-09-03:58:35\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-09-03:58:35\n",
      "INFO:tensorflow:Saving dict for global step 100: accuracy = 0.8995, global_step = 100, loss = 0.3265907\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 100: 02_QAT_MNIST/model.ckpt-100\n",
      "Classification accuracy: 89.95%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 100 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.5188952, step = 100\n",
      "INFO:tensorflow:Saving checkpoints for 200 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.6000696.\n",
      "Time Take per Iternation of training is : 93.342955%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-09-03:58:37\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-09-03:58:37\n",
      "INFO:tensorflow:Saving dict for global step 200: accuracy = 0.9307, global_step = 200, loss = 0.22154722\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 200: 02_QAT_MNIST/model.ckpt-200\n",
      "Classification accuracy: 93.07%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 200 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.4673444, step = 200\n",
      "INFO:tensorflow:Saving checkpoints for 300 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.30352902.\n",
      "Time Take per Iternation of training is : 84.700330%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-09-03:58:38\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-300\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-09-03:58:38\n",
      "INFO:tensorflow:Saving dict for global step 300: accuracy = 0.9434, global_step = 300, loss = 0.18144147\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 300: 02_QAT_MNIST/model.ckpt-300\n",
      "Classification accuracy: 94.34%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-300\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 300 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.38697818, step = 300\n",
      "INFO:tensorflow:Saving checkpoints for 400 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.26141793.\n",
      "Time Take per Iternation of training is : 84.067112%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-09-03:58:39\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-09-03:58:39\n",
      "INFO:tensorflow:Saving dict for global step 400: accuracy = 0.9533, global_step = 400, loss = 0.15691394\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 400: 02_QAT_MNIST/model.ckpt-400\n",
      "Classification accuracy: 95.33%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 400 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.32884556, step = 400\n",
      "INFO:tensorflow:Saving checkpoints for 500 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.17240058.\n",
      "Time Take per Iternation of training is : 90.871000%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-09-03:58:40\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-09-03:58:41\n",
      "INFO:tensorflow:Saving dict for global step 500: accuracy = 0.958, global_step = 500, loss = 0.14162917\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 500: 02_QAT_MNIST/model.ckpt-500\n",
      "Classification accuracy: 95.80%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 500 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.18847477, step = 500\n",
      "INFO:tensorflow:Saving checkpoints for 600 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.18113546.\n",
      "Time Take per Iternation of training is : 84.935151%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-09-03:58:42\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-600\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-09-03:58:42\n",
      "INFO:tensorflow:Saving dict for global step 600: accuracy = 0.9603, global_step = 600, loss = 0.13321371\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 600: 02_QAT_MNIST/model.ckpt-600\n",
      "Classification accuracy: 96.03%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-600\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 600 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.13987595, step = 600\n",
      "INFO:tensorflow:Saving checkpoints for 700 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.2439577.\n",
      "Time Take per Iternation of training is : 84.956086%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-09-03:58:43\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-700\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-09-03:58:43\n",
      "INFO:tensorflow:Saving dict for global step 700: accuracy = 0.9629, global_step = 700, loss = 0.12387318\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 700: 02_QAT_MNIST/model.ckpt-700\n",
      "Classification accuracy: 96.29%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-700\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 700 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.14209977, step = 700\n",
      "INFO:tensorflow:Saving checkpoints for 800 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.26769197.\n",
      "Time Take per Iternation of training is : 87.839914%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-09-03:58:44\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-800\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-09-03:58:44\n",
      "INFO:tensorflow:Saving dict for global step 800: accuracy = 0.9655, global_step = 800, loss = 0.11290719\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 800: 02_QAT_MNIST/model.ckpt-800\n",
      "Classification accuracy: 96.55%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-800\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 800 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.2543837, step = 800\n",
      "INFO:tensorflow:Saving checkpoints for 900 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.1251243.\n",
      "Time Take per Iternation of training is : 81.042652%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-09-03:58:45\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-900\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-09-03:58:46\n",
      "INFO:tensorflow:Saving dict for global step 900: accuracy = 0.9664, global_step = 900, loss = 0.11040006\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 900: 02_QAT_MNIST/model.ckpt-900\n",
      "Classification accuracy: 96.64%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-900\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 900 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.22126886, step = 900\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.10305347.\n",
      "Time Take per Iternation of training is : 88.569966%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-09-03:58:47\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-09-03:58:47\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.9691, global_step = 1000, loss = 0.10209609\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: 02_QAT_MNIST/model.ckpt-1000\n",
      "Classification accuracy: 96.91%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.14362758, step = 1000\n",
      "INFO:tensorflow:Saving checkpoints for 1100 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.10665321.\n",
      "Time Take per Iternation of training is : 87.347272%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-09-03:58:48\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-1100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-09-03:58:48\n",
      "INFO:tensorflow:Saving dict for global step 1100: accuracy = 0.9688, global_step = 1100, loss = 0.09925353\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1100: 02_QAT_MNIST/model.ckpt-1100\n",
      "Classification accuracy: 96.88%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-1100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1100 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.13009362, step = 1100\n",
      "INFO:tensorflow:Saving checkpoints for 1200 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.14212774.\n",
      "Time Take per Iternation of training is : 85.928681%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-09-03:58:49\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-1200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-09-03:58:49\n",
      "INFO:tensorflow:Saving dict for global step 1200: accuracy = 0.97, global_step = 1200, loss = 0.09739653\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1200: 02_QAT_MNIST/model.ckpt-1200\n",
      "Classification accuracy: 97.00%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-1200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1200 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.13111305, step = 1200\n",
      "INFO:tensorflow:Saving checkpoints for 1300 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.13392714.\n",
      "Time Take per Iternation of training is : 92.585536%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-09-03:58:50\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-1300\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-09-03:58:51\n",
      "INFO:tensorflow:Saving dict for global step 1300: accuracy = 0.9718, global_step = 1300, loss = 0.09688443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1300: 02_QAT_MNIST/model.ckpt-1300\n",
      "Classification accuracy: 97.18%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-1300\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1300 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.20432225, step = 1300\n",
      "INFO:tensorflow:Saving checkpoints for 1400 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.062309563.\n",
      "Time Take per Iternation of training is : 82.913801%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-09-03:58:52\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-1400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-09-03:58:52\n",
      "INFO:tensorflow:Saving dict for global step 1400: accuracy = 0.9726, global_step = 1400, loss = 0.09328024\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1400: 02_QAT_MNIST/model.ckpt-1400\n",
      "Classification accuracy: 97.26%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-1400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1400 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.09371805, step = 1400\n",
      "INFO:tensorflow:Saving checkpoints for 1500 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.19403268.\n",
      "Time Take per Iternation of training is : 83.041917%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-09-03:58:53\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-1500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-09-03:58:53\n",
      "INFO:tensorflow:Saving dict for global step 1500: accuracy = 0.9723, global_step = 1500, loss = 0.09002921\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1500: 02_QAT_MNIST/model.ckpt-1500\n",
      "Classification accuracy: 97.23%"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Key conv1-layer/act_quant/conv1-layer/act_quant/min/biased not found in checkpoint",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-173-90968467de47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conv1-layer/bias\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conv1-layer/act_quant/conv1-layer/act_quant/min/biased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conv1-layer/bias\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conv1-layer/act_quant/conv1-layer/act_quant/min/local_step'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conv1-layer/bias\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conv1-layer/act_quant/max'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/1-8/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mget_variable_value\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0m_check_checkpoint_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_variable_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/1-8/lib/python3.6/site-packages/tensorflow/python/training/checkpoint_utils.py\u001b[0m in \u001b[0;36mload_variable\u001b[0;34m(ckpt_dir_or_file, name)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m   \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_dir_or_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/1-8/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mget_tensor\u001b[0;34m(self, tensor_str)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         return CheckpointReader_GetTensor(self, compat.as_bytes(tensor_str),\n\u001b[0;32m--> 371\u001b[0;31m                                           status)\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0m__swig_destroy__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_CheckpointReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/1-8/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key conv1-layer/act_quant/conv1-layer/act_quant/min/biased not found in checkpoint"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "tf.gfile.DeleteRecursively(\"02_QAT_MNIST\");tf.gfile.MakeDirs(\"02_QAT_MNIST\")\n",
    "tf.summary.FileWriterCache.clear()\n",
    "\n",
    "model = tf.estimator.Estimator(model_fn=cnn_model_fn,\n",
    "                               params={\"learning_rate\": 1e-4},\n",
    "                               model_dir=\"02_QAT_MNIST\")\n",
    "\n",
    "import timeit\n",
    "\n",
    "EPOCHS = 15\n",
    "STEP_SIZE = 100\n",
    "count = 0\n",
    "\n",
    "while (count < EPOCHS):\n",
    "    start_time = timeit.default_timer()\n",
    "    model.train(input_fn=train_input_fn, steps=STEP_SIZE)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    sys.stdout.write(\"Time Take per Iternation of training is : {0:%}\".format(elapsed))\n",
    "\n",
    "    result = model.evaluate(input_fn=val_input_fn)\n",
    "    #print(result)\n",
    "    sys.stdout.write(\"Classification accuracy: {0:.2%}\".format(result[\"accuracy\"]))\n",
    "    sys.stdout.flush()\n",
    "    count = count + 1\n",
    "\n",
    "print(\"conv1-layer/bias\", model.get_variable_value('conv1-layer/act_quant/conv1-layer/act_quant/min/biased'))\n",
    "print(\"conv1-layer/bias\", model.get_variable_value('conv1-layer/act_quant/conv1-layer/act_quant/min/local_step'))\n",
    "print(\"conv1-layer/bias\", model.get_variable_value('conv1-layer/act_quant/max'))\n",
    "print(\"conv1-layer/bias\", model.get_variable_value('conv1-layer/act_quant/min'))\n",
    "print(\"conv1-layer/bias\", model.get_variable_value('conv1-layer/bias'))\n",
    "print(\"conv1-layer/bias\", model.get_variable_value('conv1-layer/kernel'))\n",
    "print(\"conv1-layer/bias\", model.get_variable_value('conv1-layer/weights_quant/max'))\n",
    "print(\"conv1-layer/bias\", model.get_variable_value('conv1-layer/weights_quant/min'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-5000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./02_QAT_MNIST/export/temp-b'1546996977'/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'./02_QAT_MNIST/export/1546996977'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_serving_input_receiver_fn():\n",
    "    inputs = {'image': tf.placeholder(shape=[1,784], dtype=tf.float32, name='image')}\n",
    "    return tf.estimator.export.build_raw_serving_input_receiver_fn(inputs)\n",
    "\n",
    "if tf.gfile.Exists(export_dir):\n",
    "        tf.gfile.DeleteRecursively(export_dir)\n",
    "\n",
    "model.export_savedmodel(export_dir_base=export_dir,serving_input_receiver_fn=make_serving_input_receiver_fn(),as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./02_QAT_MNIST/export/1546996977/variables/variables\n"
     ]
    }
   ],
   "source": [
    "saved_model_dir = os.path.join(export_dir, os.listdir(export_dir)[-1]) \n",
    "\n",
    "predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "  export_dir = saved_model_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Result: Pred. No. : 7')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy\n",
    "import random\n",
    "\n",
    "dir = target + \"/PNGs/\"\n",
    "pngFname = dir + random.choice(os.listdir(dir))\n",
    "\n",
    "pngFile=open(pngFname,'rb')\n",
    "png_string = pngFile.read();\n",
    "pngFile.close()\n",
    "\n",
    "#sample = tf.cast(tf.image.decode_png(png_string, channels=1), tf.int8)\n",
    "sample = tf.cast(tf.image.decode_png(png_string, channels=1), tf.float32)\n",
    "\n",
    "a = np.ones(shape=(28,28,1),dtype=np.float32)    \n",
    "\n",
    "img = tf.reshape(sample, [28, 28, 1])\n",
    "\n",
    "with sess.as_default():\n",
    "    a = img.eval()\n",
    "\n",
    "b = np.ones(shape=(1,784),dtype=np.float32)\n",
    "\n",
    "b = a.reshape((1,784))\n",
    "\n",
    "output = predictor_fn({'image': b})\n",
    "#print(output)\n",
    "#print (\"train_data.shape: \" + str(sample.get_shape()))\n",
    "#print (\"train_data.shape: \" + str(img.get_shape()))\n",
    "\n",
    "max = 0\n",
    "\n",
    "for x in range(10):\n",
    "    if output['probabilities'][0][x] >= output['probabilities'][0][max]:\n",
    "        max = x\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.imshow(a.reshape(img_shape), cmap='binary')\n",
    "xlabel = \"Result: Pred. No. : {0}\".format(max)\n",
    "ax.set_title(xlabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./02_QAT_MNIST/export/1546996977/variables/variables\n",
      "INFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}\n",
      "INFO:tensorflow:input tensors info: \n",
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: image\n",
      "INFO:tensorflow: tensor name: image_14:0, shape: (-1, 784), type: DT_FLOAT\n",
      "INFO:tensorflow:output tensors info: \n",
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: classes\n",
      "INFO:tensorflow: tensor name: ArgMax:0, shape: (-1), type: DT_INT64\n",
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: probabilities\n",
      "INFO:tensorflow: tensor name: softmax_tensor:0, shape: (-1, 10), type: DT_FLOAT\n",
      "INFO:tensorflow:Restoring parameters from ./02_QAT_MNIST/export/1546996977/variables/variables\n",
      "INFO:tensorflow:Froze 8 variables.\n",
      "INFO:tensorflow:Converted 8 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "saved_model_dir = os.path.join(export_dir, os.listdir(export_dir)[-1]) \n",
    "\n",
    "converter = tf.contrib.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "\n",
    "# converter.inference_type = tf.contrib.lite.constants.QUANTIZED_UINT8\n",
    "converter.inference_input_type = tf.contrib.lite.constants.FLOAT\n",
    "converter.output_format = tf.contrib.lite.constants.GRAPHVIZ_DOT\n",
    "#converter.output_format = tf.contrib.lite.constants.TFLITE\n",
    "input_arrays = converter.get_input_arrays()\n",
    "converter.quantized_input_stats = {input_arrays[0] : (0., 1.)}  # mean, std_dev\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "if (converter.output_format == tf.contrib.lite.constants.TFLITE):\n",
    "    open(\"converted_model.tflite\", \"wb\").write(tflite_model)\n",
    "\n",
    "if (converter.output_format == tf.contrib.lite.constants.GRAPHVIZ_DOT):\n",
    "    open(\"converted_model.graphdot\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
