{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 03: Writing first EMNIST Program\n",
    "By Rahul GAWAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This program is inspired by [GitHub](https://github.com/Hvass-Labs/TensorFlow-Tutorials)\n",
    "\n",
    "Python used: 3.6\n",
    "TensorFlow version: 1.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import urllib.request\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_w = 28\n",
    "img_h = 28\n",
    "img_shape = (img_w, img_h)\n",
    "img_shape_storage = (img_w, img_h, 1)\n",
    "\n",
    "TRAIN_VAL_RATIO = 80\n",
    "\n",
    "DATASET_DIR = '/home/rahul/ML/SSD/emnist/'\n",
    "\n",
    "dataset_index_label = '/home/rahul/ML/SSD/emnist/dataset_index_label.csv'\n",
    "dataset_fname_index = '/home/rahul/ML/SSD/emnist/dataset_fname_index.csv'\n",
    "dataset_fname_index_shuffle = '/home/rahul/ML/SSD/emnist/dataset_fname_index_shuffle.csv'\n",
    "dataset_fname_index_train = '/home/rahul/ML/SSD/emnist/dataset_fname_index_train.csv'\n",
    "dataset_fname_index_val = '/home/rahul/ML/SSD/emnist/dataset_fname_index_val.csv'\n",
    "\n",
    "training_recname = '%s/emnist_%s.tfrecord' % (DATASET_DIR, x_train_file)\n",
    "testing_recname = '%s/emnist_%s.tfrecord' % (DATASET_DIR, x_test_file)\n",
    "\n",
    "export_dir = os.path.join('./02_QAT_MNIST/', 'export')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Files:  372451  and labels:  ['O', 'R', 'Q', 'C', 'U', 'X', 'F', 'B', 'A', 'M', 'H', 'P', 'J', 'K', 'I', 'T', 'Y', 'E', 'L', 'V', 'Z', 'D', 'N', 'S', 'G', 'W'] 26\n",
      "Files are Ready !!\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "count = 0\n",
    "label = []\n",
    "\n",
    "indexFile = open(dataset_index_label, 'w', encoding=\"utf-8\")\n",
    "fnameFile = open(dataset_fname_index, 'w', encoding=\"utf-8\")\n",
    "\n",
    "for sdir in os.listdir(DATASET_DIR + '/emnist/'):\n",
    "    indexFile.write(sdir+\"\\n\")\n",
    "    label.append(sdir)\n",
    "#     print(\"Processing Directiory: \", DATASET_DIR+sdir)\n",
    "    for file in os.listdir(DATASET_DIR  + '/emnist/' + sdir):\n",
    "        row = DATASET_DIR+sdir+file + \",\" + str(index) + \"\\n\"\n",
    "        fnameFile.write(row)\n",
    "        count = count + 1\n",
    "    index = index + 1\n",
    "           \n",
    "indexFile.close()\n",
    "fnameFile.close()\n",
    "\n",
    "print(\"Added Files: \", count, \" and labels: \", label, index)\n",
    "\n",
    "fnameFile = open(dataset_fname_index, 'r', encoding=\"utf-8\")\n",
    "fnameFileShuffle = open(dataset_fname_index_shuffle, 'w', encoding=\"utf-8\")\n",
    "fnameTrain = open(dataset_fname_index_train, 'w', encoding=\"utf-8\")\n",
    "fnameVal = open(dataset_fname_index_val, 'w', encoding=\"utf-8\")\n",
    "\n",
    "lis=[line.split() for line in fnameFile]\n",
    "random.shuffle(lis)\n",
    "\n",
    "writer = csv.writer(fnameFileShuffle)\n",
    "\n",
    "for item in lis:\n",
    "    row = item[0].split(\",\")\n",
    "    rowsdtr = row[0] + \",\" + row[1] + \"\\n\"\n",
    "    fnameFileShuffle.write(rowsdtr)\n",
    "\n",
    "total = len(lis)\n",
    "TrainingEnd = int((total*TRAIN_VAL_RATIO)/100)\n",
    "\n",
    "for count in range(0, TrainingEnd):\n",
    "    item = lis[count]\n",
    "    row = item[0].split(\",\")\n",
    "    rowsdtr = row[0] + \",\" + row[1] + \"\\n\"\n",
    "    fnameTrain.write(rowsdtr)\n",
    "\n",
    "for count in range(TrainingEnd, total):\n",
    "    item = lis[count]\n",
    "    row = item[0].split(\",\")\n",
    "    rowsdtr = row[0] + \",\" + row[1] + \"\\n\"\n",
    "    fnameVal.write(rowsdtr)\n",
    "\n",
    "fnameFileShuffle.close()\n",
    "fnameFile.close()\n",
    "print(\"Files are Ready !!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset to a local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x_train Set 60000\n",
      "y_train_cls Set 60000\n",
      "x_test Set 10000\n",
      "y_test_cls Set 10000\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "\n",
    "#save_path = os.path.join(download_dir, filename)\n",
    "\n",
    "x_train = download_img(x_train_url, x_train_file, target,16)\n",
    "print(\"x_train Set\", len(x_train))\n",
    "\n",
    "y_train_cls = download_cls(y_train_url, y_train_file, target,8)\n",
    "print(\"y_train_cls Set\", len(y_train_cls))\n",
    "\n",
    "x_test = download_img(x_test_url, x_test_file, target,16)\n",
    "print(\"x_test Set\", len(x_test))\n",
    "\n",
    "y_test_cls = download_cls(y_test_url, y_test_file, target,8)\n",
    "print(\"y_test_cls Set\", len(y_test_cls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set 60000  images and  60000  classes.\n",
      "Validation Set 0  images and  0  classes.\n",
      "Test Set 10000  images and  10000  classes.\n"
     ]
    }
   ],
   "source": [
    "num_train = 60000\n",
    "num_val = 5000\n",
    "num_test = 10000\n",
    "\n",
    "X_train = x_train[0:num_train] / 255.0\n",
    "Y_train_cls = y_train_cls[0:num_train]\n",
    "\n",
    "X_val = x_train[num_train:] / 255.0\n",
    "Y_val_cls = y_train_cls[num_train:]\n",
    "\n",
    "print(\"Training Set\", len(X_train), \" images and \", len(Y_train_cls), \" classes.\")\n",
    "print(\"Validation Set\", len(X_val), \" images and \", len(Y_val_cls), \" classes.\")\n",
    "\n",
    "X_test = x_test[0:num_test] / 255.0\n",
    "Y_test_cls = y_test_cls[0:num_test]\n",
    "\n",
    "print(\"Test Set\", len(X_test), \" images and \", len(Y_test_cls), \" classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None, img_w=28, img_h=28):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "\n",
    "\n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the first images from the test-set.\n",
    "images = X_test[0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = Y_test_cls[0:9]\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images=images, cls_true=cls_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_cls_char = Y_train_cls.astype(np.int)\n",
    "y_val_cls_char = Y_val_cls.astype(np.int)\n",
    "y_test_cls_char = Y_test_cls.astype(np.int)\n",
    "\n",
    "Y_train_OHE = np.eye(10, dtype=float)[y_train_cls_char]\n",
    "Y_val_OHE = np.eye(10, dtype=float)[y_val_cls_char]\n",
    "Y_test_OHE = np.eye(10, dtype=float)[y_test_cls_char]\n",
    "\n",
    "Y_test_OHE[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-record (Optional)\n",
    "\n",
    "[Referred this implementation to create tf.record](https://github.com/tensorflow/models/blob/master/research/slim/datasets/download_and_convert_mnist.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int64_feature(values):\n",
    "  if not isinstance(values, (tuple, list)):\n",
    "    values = [values]\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tfexample(image_data, image_format, height, width, class_id):\n",
    "  return tf.train.Example(features=tf.train.Features(feature={\n",
    "      'image/encoded':  tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_data])),\n",
    "      'image/format':  tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_format])),\n",
    "      'image/class/label': int64_feature(class_id),\n",
    "      'image/height': int64_feature(height),\n",
    "      'image/width': int64_feature(width)\n",
    "  }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _add_to_tfrecord(data_filename, labels_filename, num_images, tfrecord_writer, images, labels, dump):\n",
    "    with tf.Graph().as_default():\n",
    "        image = tf.placeholder(dtype=tf.uint8, shape=img_shape_storage)\n",
    "        encoded_png = tf.image.encode_png(image)\n",
    "\n",
    "        num_images = len(images)\n",
    "        images = images.reshape(-1, img_w, img_h, 1)\n",
    "\n",
    "        with tf.Session('') as sess:\n",
    "            for j in range(num_images):\n",
    "                sys.stdout.write('\\r>> Converting image %d/%d' % (j + 1, num_images))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                png_string = sess.run(encoded_png, feed_dict={image: images[j]})\n",
    "                example = image_to_tfexample(png_string, 'png'.encode(), img_w, img_h, labels[j])\n",
    "                tfrecord_writer.write(example.SerializeToString())\n",
    "\n",
    "                if dump == True:\n",
    "                    pngFname = target + \"/PNGs/\" + \"train_%d.png\" % (j)\n",
    "                    pngFile=open(pngFname,'wb')\n",
    "                    pngFile.write(png_string);\n",
    "                    pngFile.close()\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train Set 60000\n",
      "y_train_cls Set 60000\n",
      "x_test Set 10000\n",
      "y_test_cls Set 10000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "x_train = download_img(x_train_url, x_train_file, target,16)\n",
    "print(\"x_train Set\", len(x_train))\n",
    "\n",
    "y_train_cls = download_cls(y_train_url, y_train_file, target,8)\n",
    "print(\"y_train_cls Set\", len(y_train_cls))\n",
    "\n",
    "x_test = download_img(x_test_url, x_test_file, target,16)\n",
    "print(\"x_test Set\", len(x_test))\n",
    "\n",
    "y_test_cls = download_cls(y_test_url, y_test_file, target,8)\n",
    "print(\"y_test_cls Set\", len(y_test_cls))\n",
    "\n",
    "print(len(x_train))\n",
    "\n",
    "if not os.path.exists(training_recname):\n",
    "    with tf.python_io.TFRecordWriter(training_recname) as tfrecord_writer:\n",
    "        _add_to_tfrecord(x_train_rpath, y_train_rpath, 60000, tfrecord_writer, x_train, y_train_cls, False)\n",
    "\n",
    "if not os.path.exists(testing_recname):\n",
    "    with tf.python_io.TFRecordWriter(testing_recname) as tfrecord_writer:\n",
    "        _add_to_tfrecord(x_test_rpath, y_test_rpath, 10000, tfrecord_writer, x_test, y_test_cls, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(record):\n",
    "    keys_to_features = {\n",
    "        \"image/encoded\":  tf.FixedLenFeature([], tf.string),\n",
    "        \"image/format\":  tf.FixedLenFeature([], tf.string),\n",
    "        \"image/class/label\": tf.FixedLenFeature([], tf.int64),\n",
    "        \"image/height\": tf.FixedLenFeature([], tf.int64),\n",
    "        \"image/width\": tf.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "\n",
    "    parsed = tf.parse_single_example(record, keys_to_features)\n",
    "\n",
    "    height = tf.cast(parsed[\"image/height\"], tf.int32)\n",
    "    width = tf.cast(parsed[\"image/width\"], tf.int32)\n",
    "    label = tf.cast(parsed[\"image/class/label\"], tf.int32)\n",
    "    image = tf.cast(tf.image.decode_png(parsed[\"image/encoded\"], channels=1), tf.float32)\n",
    "    \n",
    " \n",
    "    return {'image': image}, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(filenames):\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "    THREADS = 4\n",
    "    PREFETCH = 64\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(filenames=filenames, num_parallel_reads=THREADS)\n",
    "    dataset = dataset.apply(\n",
    "        tf.contrib.data.shuffle_and_repeat(1024, 1)\n",
    "    )\n",
    "    dataset = dataset.apply(\n",
    "        tf.contrib.data.map_and_batch(parser, BATCH_SIZE)\n",
    "    )\n",
    "\n",
    "    dataset = dataset.prefetch(buffer_size=PREFETCH)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    return input_fn(filenames=[training_recname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_input_fn():\n",
    "    return input_fn(filenames=[testing_recname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_val_images():\n",
    "\n",
    "    BATCH_SIZE = 10000\n",
    "    THREADS = 4\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames=[testing_recname], num_parallel_reads=THREADS)\n",
    "    dataset = dataset.apply(\n",
    "        tf.contrib.data.map_and_batch(parser, BATCH_SIZE)\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode, params):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer\n",
    "    # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "    # MNIST images are 28x28 pixels, and have one color channel\n",
    "\n",
    "    # Print all of the operations in the default graph.\n",
    "\n",
    "    net = features[\"image\"]\n",
    "\n",
    "    net = tf.identity(net, name=\"input_tensor\")\n",
    "\n",
    "    net = tf.reshape(net, [-1, 28, 28, 1])\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "    # Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    conv1 = tf.layers.conv2d(inputs=net, filters=32,kernel_size=[5, 5],padding=\"same\", activation=tf.nn.relu, name=\"conv1-layer\")\n",
    "    \n",
    "    # Pooling Layer #1\n",
    "    # First max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2, name=\"pool1-layer\")\n",
    "\n",
    "    # Convolutional Layer #2\n",
    "    # Computes 64 features using a 5x5 filter.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    conv2 = tf.layers.conv2d(inputs=pool1,filters=64,kernel_size=[5, 5],padding=\"same\",activation=tf.nn.relu, name=\"conv2-layer\")\n",
    "\n",
    "    # Pooling Layer #2\n",
    "    # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2, name=\"pool2-layer\")\n",
    "\n",
    "    # Flatten tensor into a batch of vectors\n",
    "    # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "    # Dense Layer\n",
    "    # Densely connected layer with 1024 neurons\n",
    "    # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    # Output Tensor Shape: [batch_size, 1024]\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu, name=\"dense-layer\")\n",
    "\n",
    "    # Add dropout operation; 0.6 probability that element will be kept\n",
    "    dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits layer\n",
    "    # Input Tensor Shape: [batch_size, 1024]\n",
    "    # Output Tensor Shape: [batch_size, 10]\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1), \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "#     if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "#         # Call rewriter to produce graph with fake quant ops and folded batch norms\n",
    "#         # quant_delay delays start of quantization till quant_delay steps, allowing\n",
    "#         # for better model accuracy.\n",
    "#         g = tf.get_default_graph()\n",
    "#         tf.contrib.quantize.create_training_graph(input_graph=g, quant_delay=1000)\n",
    "\n",
    "#     if mode == tf.estimator.ModeKeys.EVAL:\n",
    "#         # Call the eval rewrite which rewrites the graph in-place with\n",
    "#         # FakeQuantization nodes and fold batchnorm for eval.\n",
    "#         g = tf.get_default_graph()\n",
    "#         tf.contrib.quantize.create_eval_graph(input_graph=g)     \n",
    "\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(loss=loss,global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])\n",
    "    }\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '02_QAT_MNIST', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f45fb2b75c0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From <ipython-input-15-545d84d7a83e>:9: shuffle_and_repeat (from tensorflow.contrib.data.python.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.shuffle_and_repeat(...)`.\n",
      "WARNING:tensorflow:From <ipython-input-15-545d84d7a83e>:12: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.map_and_batch(...)`.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 24.680899, step = 0\n",
      "INFO:tensorflow:global_step/sec: 392.004\n",
      "INFO:tensorflow:loss = 0.6270354, step = 100 (0.256 sec)\n",
      "INFO:tensorflow:global_step/sec: 513.806\n",
      "INFO:tensorflow:loss = 0.42869622, step = 200 (0.195 sec)\n",
      "INFO:tensorflow:global_step/sec: 525.835\n",
      "INFO:tensorflow:loss = 0.3914283, step = 300 (0.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 524.712\n",
      "INFO:tensorflow:loss = 0.26112044, step = 400 (0.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 531.819\n",
      "INFO:tensorflow:loss = 0.31908187, step = 500 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 526.116\n",
      "INFO:tensorflow:loss = 0.09347874, step = 600 (0.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 530.261\n",
      "INFO:tensorflow:loss = 0.22068919, step = 700 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 503.828\n",
      "INFO:tensorflow:loss = 0.16974714, step = 800 (0.199 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 900 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.03394115.\n",
      "Time Take per Iternation of training is : 450.881487%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-15-06:39:11\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-900\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-15-06:39:11\n",
      "INFO:tensorflow:Saving dict for global step 900: accuracy = 0.9653, global_step = 900, loss = 0.108613156\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 900: 02_QAT_MNIST/model.ckpt-900\n",
      "Classification accuracy: 96.53%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-900\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 900 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.33134115, step = 900\n",
      "INFO:tensorflow:global_step/sec: 422.84\n",
      "INFO:tensorflow:loss = 0.14768738, step = 1000 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 522.939\n",
      "INFO:tensorflow:loss = 0.24869412, step = 1100 (0.191 sec)\n",
      "INFO:tensorflow:global_step/sec: 530.6\n",
      "INFO:tensorflow:loss = 0.08825572, step = 1200 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 528.789\n",
      "INFO:tensorflow:loss = 0.06613192, step = 1300 (0.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 520.419\n",
      "INFO:tensorflow:loss = 0.1469255, step = 1400 (0.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 525.984\n",
      "INFO:tensorflow:loss = 0.12857556, step = 1500 (0.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 526.309\n",
      "INFO:tensorflow:loss = 0.14572237, step = 1600 (0.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 534.776\n",
      "INFO:tensorflow:loss = 0.05501428, step = 1700 (0.187 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1800 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.052981753.\n",
      "Time Take per Iternation of training is : 218.249970%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-15-06:39:13\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-1800\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-15-06:39:14\n",
      "INFO:tensorflow:Saving dict for global step 1800: accuracy = 0.976, global_step = 1800, loss = 0.07605916\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1800: 02_QAT_MNIST/model.ckpt-1800\n",
      "Classification accuracy: 97.60%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-1800\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1800 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.10763915, step = 1800\n",
      "INFO:tensorflow:global_step/sec: 424.987\n",
      "INFO:tensorflow:loss = 0.13254279, step = 1900 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 529.059\n",
      "INFO:tensorflow:loss = 0.200082, step = 2000 (0.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 527.522\n",
      "INFO:tensorflow:loss = 0.049762838, step = 2100 (0.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 529.659\n",
      "INFO:tensorflow:loss = 0.17837223, step = 2200 (0.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 525.489\n",
      "INFO:tensorflow:loss = 0.15087402, step = 2300 (0.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 526.131\n",
      "INFO:tensorflow:loss = 0.16669758, step = 2400 (0.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 526.518\n",
      "INFO:tensorflow:loss = 0.017764421, step = 2500 (0.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 517.61\n",
      "INFO:tensorflow:loss = 0.05913885, step = 2600 (0.193 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2700 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.13807683.\n",
      "Time Take per Iternation of training is : 223.819871%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-15-06:39:16\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-2700\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-15-06:39:16\n",
      "INFO:tensorflow:Saving dict for global step 2700: accuracy = 0.9793, global_step = 2700, loss = 0.065704234\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2700: 02_QAT_MNIST/model.ckpt-2700\n",
      "Classification accuracy: 97.93%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-2700\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2700 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.057824917, step = 2700\n",
      "INFO:tensorflow:global_step/sec: 431.368\n",
      "INFO:tensorflow:loss = 0.2542783, step = 2800 (0.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 532.065\n",
      "INFO:tensorflow:loss = 0.089419074, step = 2900 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 528.282\n",
      "INFO:tensorflow:loss = 0.06423138, step = 3000 (0.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 530.99\n",
      "INFO:tensorflow:loss = 0.029868348, step = 3100 (0.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 529.153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.100378916, step = 3200 (0.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 525.222\n",
      "INFO:tensorflow:loss = 0.028328758, step = 3300 (0.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 523.268\n",
      "INFO:tensorflow:loss = 0.05341384, step = 3400 (0.191 sec)\n",
      "INFO:tensorflow:global_step/sec: 528.359\n",
      "INFO:tensorflow:loss = 0.012371156, step = 3500 (0.189 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3600 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.059472412.\n",
      "Time Take per Iternation of training is : 221.222856%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-15-06:39:19\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-3600\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-15-06:39:19\n",
      "INFO:tensorflow:Saving dict for global step 3600: accuracy = 0.982, global_step = 3600, loss = 0.057926353\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3600: 02_QAT_MNIST/model.ckpt-3600\n",
      "Classification accuracy: 98.20%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-3600\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3600 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.15406094, step = 3600\n",
      "INFO:tensorflow:global_step/sec: 431.329\n",
      "INFO:tensorflow:loss = 0.02154551, step = 3700 (0.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 522.199\n",
      "INFO:tensorflow:loss = 0.118769035, step = 3800 (0.191 sec)\n",
      "INFO:tensorflow:global_step/sec: 522.089\n",
      "INFO:tensorflow:loss = 0.0062951604, step = 3900 (0.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 522.506\n",
      "INFO:tensorflow:loss = 0.13219962, step = 4000 (0.191 sec)\n",
      "INFO:tensorflow:global_step/sec: 521.206\n",
      "INFO:tensorflow:loss = 0.030053934, step = 4100 (0.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 518.223\n",
      "INFO:tensorflow:loss = 0.024730282, step = 4200 (0.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 527.727\n",
      "INFO:tensorflow:loss = 0.0091509, step = 4300 (0.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 527.757\n",
      "INFO:tensorflow:loss = 0.009983968, step = 4400 (0.190 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4500 into 02_QAT_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.05518607.\n",
      "Time Take per Iternation of training is : 218.865598%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-15-06:39:21\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-4500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-15-06:39:21\n",
      "INFO:tensorflow:Saving dict for global step 4500: accuracy = 0.9841, global_step = 4500, loss = 0.051180307\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4500: 02_QAT_MNIST/model.ckpt-4500\n",
      "Classification accuracy: 98.41%"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if os.path.exists(\"02_QAT_MNIST\"):\n",
    "    tf.gfile.DeleteRecursively(\"02_QAT_MNIST\");tf.gfile.MakeDirs(\"02_QAT_MNIST\")\n",
    "    tf.summary.FileWriterCache.clear()\n",
    "\n",
    "model = tf.estimator.Estimator(model_fn=cnn_model_fn,\n",
    "                               params={\"learning_rate\": 1e-4},\n",
    "                               model_dir=\"02_QAT_MNIST\")\n",
    "\n",
    "import timeit\n",
    "\n",
    "EPOCHS = 5\n",
    "STEP_SIZE = 900\n",
    "count = 0\n",
    "\n",
    "while (count < EPOCHS):\n",
    "    start_time = timeit.default_timer()\n",
    "    model.train(input_fn=train_input_fn, steps=STEP_SIZE)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    sys.stdout.write(\"Time Take per Iternation of training is : {0:%}\".format(elapsed))\n",
    "\n",
    "    result = model.evaluate(input_fn=val_input_fn)\n",
    "    #print(result)\n",
    "    sys.stdout.write(\"Classification accuracy: {0:.2%}\".format(result[\"accuracy\"]))\n",
    "    sys.stdout.flush()\n",
    "    count = count + 1\n",
    "\n",
    "# print(\"conv1-layer/bias\", model.get_variable_value('conv1-layer/act_quant/conv1-layer/act_quant/min/biased'))\n",
    "# print(\"conv1-layer/bias\", model.get_variable_value('conv1-layer/act_quant/conv1-layer/act_quant/min/local_step'))\n",
    "# print(\"conv1-layer/bias\", model.get_variable_value('conv1-layer/act_quant/max'))\n",
    "# print(\"conv1-layer/bias\", model.get_variable_value('conv1-layer/act_quant/min'))\n",
    "# print(\"conv1-layer/bias\", model.get_variable_value('conv1-layer/bias'))\n",
    "# print(\"conv1-layer/bias\", model.get_variable_value('conv1-layer/kernel'))\n",
    "# print(\"conv1-layer/bias\", model.get_variable_value('conv1-layer/weights_quant/max'))\n",
    "# print(\"conv1-layer/bias\", model.get_variable_value('conv1-layer/weights_quant/min'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from 02_QAT_MNIST/model.ckpt-4500\n",
      "WARNING:tensorflow:From /home/rahul/anaconda3/envs/1-8/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py:1044: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Pass your op to the equivalent parameter main_op instead.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./02_QAT_MNIST/export/temp-b'1547534361'/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'./02_QAT_MNIST/export/1547534361'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_serving_input_receiver_fn():\n",
    "    inputs = {'image': tf.placeholder(shape=[1,784], dtype=tf.float32, name='image')}\n",
    "    return tf.estimator.export.build_raw_serving_input_receiver_fn(inputs)\n",
    "\n",
    "if tf.gfile.Exists(export_dir):\n",
    "        tf.gfile.DeleteRecursively(export_dir)\n",
    "\n",
    "model.export_savedmodel(export_dir_base=export_dir,serving_input_receiver_fn=make_serving_input_receiver_fn(),as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./02_QAT_MNIST/export/1547534361/variables/variables\n"
     ]
    }
   ],
   "source": [
    "saved_model_dir = os.path.join(export_dir, os.listdir(export_dir)[-1]) \n",
    "\n",
    "predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "  export_dir = saved_model_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Result: Pred. No. : 1')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "35\n",
    "dir = target + \"/PNGs/\"\n",
    "pngFname = dir + random.choice(os.listdir(dir))\n",
    "\n",
    "pngFile=open(pngFname,'rb')\n",
    "png_string = pngFile.read();\n",
    "pngFile.close()\n",
    "\n",
    "#sample = tf.cast(tf.image.decode_png(png_string, channels=1), tf.int8)\n",
    "sample = tf.cast(tf.image.decode_png(png_string, channels=1), tf.float32)\n",
    "\n",
    "a = np.ones(shape=(28,28,1),dtype=np.float32)    \n",
    "\n",
    "img = tf.reshape(sample, [28, 28, 1])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    a = img.eval()\n",
    "\n",
    "b = np.ones(shape=(1,784),dtype=np.float32)\n",
    "\n",
    "b = a.reshape((1,784))\n",
    "\n",
    "output = predictor_fn({'image': b})\n",
    "#print(output)\n",
    "#print (\"train_data.shape: \" + str(sample.get_shape()))\n",
    "#print (\"train_data.shape: \" + str(img.get_shape()))\n",
    "\n",
    "max = 0\n",
    "\n",
    "for x in range(10):\n",
    "    if output['probabilities'][0][x] >= output['probabilities'][0][max]:\n",
    "        max = x\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.imshow(a.reshape(img_shape), cmap='binary')\n",
    "xlabel = \"Result: Pred. No. : {0}\".format(max)\n",
    "ax.set_title(xlabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record_iterator = tf.python_io.tf_record_iterator(path=testing_recname)\n",
    "\n",
    "\n",
    "# count = 0\n",
    "# success = 0\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     tf.train.start_queue_runners()\n",
    "\n",
    "#     for string_record in record_iterator:\n",
    "#         if count%100 == 0:\n",
    "#             print(\"Success / Total: \", success, \" / \", count, \"Accuracy: {0:.2%}\".format(success/(count+1)))\n",
    "\n",
    "#         example = tf.train.Example()\n",
    "#         example.ParseFromString(string_record)\n",
    "\n",
    "#         sample = tf.cast(tf.image.decode_png(example.features.feature['image/encoded'].bytes_list.value[0], channels=1), tf.float32)\n",
    "#         label = int(example.features.feature['image/class/label'].int64_list.value[0])\n",
    "\n",
    "#         a = np.ones(shape=(28,28,1),dtype=np.float32)    \n",
    "\n",
    "#         img = tf.reshape(sample, [28, 28, 1])\n",
    "\n",
    "#         with sess.as_default():\n",
    "#             a = img.eval()\n",
    "\n",
    "#         b = np.ones(shape=(1,784),dtype=np.float32)\n",
    "\n",
    "#         b = a.reshape((1,784))\n",
    "\n",
    "#         output = predictor_fn({'image': b})\n",
    "\n",
    "#         max = 0\n",
    "\n",
    "#         for x in range(10):\n",
    "#             if output['probabilities'][0][x] >= output['probabilities'][0][max]:\n",
    "#                 max = x\n",
    "\n",
    "#         if max == label:\n",
    "#             success = success + 1\n",
    "\n",
    "#         count = count + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./02_QAT_MNIST/export/1547534361/variables/variables\n",
      "INFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}\n",
      "INFO:tensorflow:input tensors info: \n",
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: image\n",
      "INFO:tensorflow: tensor name: image:0, shape: (-1, 784), type: DT_FLOAT\n",
      "INFO:tensorflow:output tensors info: \n",
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: probabilities\n",
      "INFO:tensorflow: tensor name: softmax_tensor:0, shape: (-1, 10), type: DT_FLOAT\n",
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: classes\n",
      "INFO:tensorflow: tensor name: ArgMax:0, shape: (-1), type: DT_INT64\n",
      "INFO:tensorflow:Restoring parameters from ./02_QAT_MNIST/export/1547534361/variables/variables\n",
      "INFO:tensorflow:Froze 8 variables.\n",
      "INFO:tensorflow:Converted 8 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "saved_model_dir = os.path.join(export_dir, os.listdir(export_dir)[-1]) \n",
    "\n",
    "converter = tf.contrib.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "\n",
    "converter.inference_type = tf.contrib.lite.constants.QUANTIZED_UINT8\n",
    "# converter.inference_input_type = tf.contrib.lite.constants.FLOAT\n",
    "# converter.output_format = tf.contrib.lite.constants.GRAPHVIZ_DOT\n",
    "converter.output_format = tf.contrib.lite.constants.TFLITE\n",
    "input_arrays = converter.get_input_arrays()\n",
    "converter.quantized_input_stats = {input_arrays[0] : (0, 1)}  # mean, std_dev\n",
    "converter.default_ranges_stats = (-127,127)\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "if (converter.output_format == tf.contrib.lite.constants.TFLITE):\n",
    "    open(\"converted_model.tflite\", \"wb\").write(tflite_model)\n",
    "\n",
    "if (converter.output_format == tf.contrib.lite.constants.GRAPHVIZ_DOT):\n",
    "    open(\"converted_model.graphdot\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_data:  [[7]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Result: Pred. No. : [[7]]')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "interpreter = tf.contrib.lite.Interpreter(model_path=\"converted_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "input_shape = input_details[0]['shape']\n",
    "\n",
    "########################################################\n",
    "\n",
    "dir = target + \"/PNGs/\"\n",
    "pngFname = dir + random.choice(os.listdir(dir))\n",
    "\n",
    "pngFile=open(pngFname,'rb')\n",
    "png_string = pngFile.read();\n",
    "pngFile.close()\n",
    "\n",
    "sample = tf.cast(tf.image.decode_png(png_string, channels=1), tf.uint8)\n",
    "\n",
    "a = np.ones(shape=(28,28,1),dtype=np.uint8)    \n",
    "\n",
    "img = tf.reshape(sample, [1, 784])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    a = img.eval()\n",
    "\n",
    "b = np.ones(shape=input_shape,dtype=np.uint8)\n",
    "b = a.reshape(input_shape)\n",
    "\n",
    "########################################################\n",
    "\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=np.uint8)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    input_data = img.eval()\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(\"output_data: \", output_data)\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.imshow(input_data.reshape(img_shape), cmap='gray')\n",
    "xlabel = \"Result: Pred. No. : {0}\".format(output_data)\n",
    "ax.set_title(xlabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success / Total:  0  /  0 Accuracy: 0.00%\n",
      "Success / Total:  981  /  1000 Accuracy: 98.00%\n",
      "Success / Total:  1950  /  2000 Accuracy: 97.45%\n",
      "Success / Total:  2914  /  3000 Accuracy: 97.10%\n",
      "Success / Total:  3898  /  4000 Accuracy: 97.43%\n",
      "Success / Total:  4868  /  5000 Accuracy: 97.34%\n",
      "Success / Total:  5860  /  6000 Accuracy: 97.65%\n",
      "Success / Total:  6847  /  7000 Accuracy: 97.80%\n",
      "Success / Total:  7843  /  8000 Accuracy: 98.03%\n",
      "Success / Total:  8838  /  9000 Accuracy: 98.19%\n",
      "Final : Success / Total:  9821  /  10000 Accuracy: 98.20%\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "record_iterator = tf.python_io.tf_record_iterator(path=testing_recname)\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=np.uint8)\n",
    "\n",
    "interpreter.reset_all_variables()\n",
    "interpreter = tf.contrib.lite.Interpreter(model_path=\"converted_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "\n",
    "count = 0\n",
    "success = 0\n",
    "\n",
    "for string_record in record_iterator:\n",
    "    if count%1000 == 0:\n",
    "        print(\"Success / Total: \", success, \" / \", count, \"Accuracy: {0:.2%}\".format(success/(count+1)))\n",
    "\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(string_record)\n",
    "\n",
    "    label = int(example.features.feature['image/class/label'].int64_list.value[0])\n",
    "\n",
    "#   sample = tf.cast(tf.image.decode_png(example.features.feature['image/encoded'].bytes_list.value[0], channels=1), tf.uint8)\n",
    "    x = np.frombuffer(example.features.feature['image/encoded'].bytes_list.value[0], dtype='uint8')\n",
    "    sample = cv2.imdecode(x, cv2.IMREAD_UNCHANGED).reshape(1,784);\n",
    "\n",
    "    interpreter.set_tensor(input_details[0]['index'], sample)\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "    if output_data[0][0] == label:\n",
    "        success = success + 1\n",
    "\n",
    "    count = count + 1\n",
    "\n",
    "print(\"Final : Success / Total: \", success, \" / \", count, \"Accuracy: {0:.2%}\".format(success/(count+1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "    keys_to_features = {\n",
    "        \"image/encoded\":  tf.FixedLenFeature([], tf.string),\n",
    "        \"image/format\":  tf.FixedLenFeature([], tf.string),\n",
    "        \"image/class/label\": tf.FixedLenFeature([], tf.int64),\n",
    "        \"image/height\": tf.FixedLenFeature([], tf.int64),\n",
    "        \"image/width\": tf.FixedLenFeature([], tf.int64)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
