{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 01: Writing first MNIST Program\n",
    "By Rahul GAWAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This program is inspired by [GitHub](https://github.com/Hvass-Labs/TensorFlow-Tutorials)\n",
    "\n",
    "Python used: 3.6\n",
    "TensorFlow version: 1.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import urllib.request\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_w = 28\n",
    "img_h = 28\n",
    "img_shape = (img_w, img_h)\n",
    "img_shape_storage = (img_w, img_h, 1)\n",
    "\n",
    "url = \"https://storage.googleapis.com/cvdf-datasets/mnist/\"\n",
    "\n",
    "x_train_file = \"train-images-idx3-ubyte.gz\"\n",
    "y_train_file = \"train-labels-idx1-ubyte.gz\"\n",
    "x_test_file = \"t10k-images-idx3-ubyte.gz\"\n",
    "y_test_file = \"t10k-labels-idx1-ubyte.gz\"\n",
    "\n",
    "target = \"mnist_dataset\"\n",
    "\n",
    "x_train_url = url + x_train_file\n",
    "y_train_url = url + y_train_file\n",
    "x_test_url = url + x_test_file\n",
    "y_test_url = url + y_test_file\n",
    "\n",
    "x_train_rpath = target + \"/\" + x_train_file\n",
    "y_train_rpath = target + \"/\" + y_train_file\n",
    "x_test_rpath = target + \"/\" + x_test_file\n",
    "y_test_rpath = target + \"/\" + y_test_file\n",
    "\n",
    "training_recname = '%s/mnist_%s.tfrecord' % (target, x_train_file)\n",
    "testing_recname = '%s/mnist_%s.tfrecord' % (target, x_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset to a local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print_download_progress(count, block_size, total_size):\n",
    "    \"\"\"\n",
    "    Function used for printing the download progress.\n",
    "    Used as a call-back function in maybe_download_and_extract().\n",
    "    \"\"\"\n",
    "\n",
    "    # Percentage completion.\n",
    "    pct_complete = float(count * block_size) / total_size\n",
    "\n",
    "    # Limit it because rounding errors may cause it to exceed 100%.\n",
    "    pct_complete = min(1.0, pct_complete)\n",
    "\n",
    "    # Status-message. Note the \\r which means the line should overwrite itself.\n",
    "    msg = \"\\r- Download progress: {0:.1%}\".format(pct_complete)\n",
    "\n",
    "    # Print it.\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_img(base_url, filename, download_dir, offset):\n",
    "\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "\n",
    "    file_path = os.path.join(download_dir, filename)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"\\nDownloading\", base_url, \"at\", download_dir, \".\")\n",
    "        file_path, _ = urllib.request.urlretrieve( base_url, file_path, _print_download_progress)\n",
    "\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=offset)\n",
    "\n",
    "    img_size = 28\n",
    "    img_size_flat = img_size * img_size\n",
    "    num_channels = 1\n",
    "    img_shape_full = (img_size, img_size, num_channels)\n",
    "    images_flat = data.reshape(-1, img_size_flat)\n",
    "    return images_flat\n",
    "\n",
    "def download_cls(base_url, filename, download_dir,  offset):\n",
    "\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "\n",
    "    file_path = os.path.join(download_dir, filename)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"\\nDownloading\", base_url, \"at\", download_dir, \".\")\n",
    "        file_path, _ = urllib.request.urlretrieve( base_url, file_path, _print_download_progress)\n",
    "\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8,  offset=offset)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x_train Set 60000\n",
      "y_train_cls Set 60000\n",
      "x_test Set 10000\n",
      "y_test_cls Set 10000\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "\n",
    "#save_path = os.path.join(download_dir, filename)\n",
    "\n",
    "x_train = download_img(x_train_url, x_train_file, target,16)\n",
    "print(\"x_train Set\", len(x_train))\n",
    "\n",
    "y_train_cls = download_cls(y_train_url, y_train_file, target,8)\n",
    "print(\"y_train_cls Set\", len(y_train_cls))\n",
    "\n",
    "x_test = download_img(x_test_url, x_test_file, target,16)\n",
    "print(\"x_test Set\", len(x_test))\n",
    "\n",
    "y_test_cls = download_cls(y_test_url, y_test_file, target,8)\n",
    "print(\"y_test_cls Set\", len(y_test_cls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set 55000  images and  55000  classes.\n",
      "Validation Set 5000  images and  5000  classes.\n",
      "Test Set 10000  images and  10000  classes.\n"
     ]
    }
   ],
   "source": [
    "num_train = 55000\n",
    "num_val = 5000\n",
    "num_test = 10000\n",
    "\n",
    "X_train = x_train[0:num_train] / 255.0\n",
    "Y_train_cls = y_train_cls[0:num_train]\n",
    "\n",
    "X_val = x_train[num_train:] / 255.0\n",
    "Y_val_cls = y_train_cls[num_train:]\n",
    "\n",
    "print(\"Training Set\", len(X_train), \" images and \", len(Y_train_cls), \" classes.\")\n",
    "print(\"Validation Set\", len(X_val), \" images and \", len(Y_val_cls), \" classes.\")\n",
    "\n",
    "X_test = x_test[0:num_test] / 255.0\n",
    "Y_test_cls = y_test_cls[0:num_test]\n",
    "\n",
    "print(\"Test Set\", len(X_test), \" images and \", len(Y_test_cls), \" classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None, img_w=28, img_h=28):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "\n",
    "\n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the first images from the test-set.\n",
    "images = X_test[0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = Y_test_cls[0:9]\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images=images, cls_true=cls_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_cls_char = Y_train_cls.astype(np.int)\n",
    "y_val_cls_char = Y_val_cls.astype(np.int)\n",
    "y_test_cls_char = Y_test_cls.astype(np.int)\n",
    "\n",
    "Y_train_OHE = np.eye(10, dtype=float)[y_train_cls_char]\n",
    "Y_val_OHE = np.eye(10, dtype=float)[y_val_cls_char]\n",
    "Y_test_OHE = np.eye(10, dtype=float)[y_test_cls_char]\n",
    "\n",
    "Y_test_OHE[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-record (Optional)\n",
    "\n",
    "[Referred this implementation to create tf.record](https://github.com/tensorflow/models/blob/master/research/slim/datasets/download_and_convert_mnist.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int64_feature(values):\n",
    "  if not isinstance(values, (tuple, list)):\n",
    "    values = [values]\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tfexample(image_data, image_format, height, width, class_id):\n",
    "  return tf.train.Example(features=tf.train.Features(feature={\n",
    "      'image/encoded':  tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_data])),\n",
    "      'image/format':  tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_format])),\n",
    "      'image/class/label': int64_feature(class_id),\n",
    "      'image/height': int64_feature(height),\n",
    "      'image/width': int64_feature(width)\n",
    "  }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _add_to_tfrecord(data_filename, labels_filename, num_images, tfrecord_writer, images, labels):\n",
    "    with tf.Graph().as_default():\n",
    "        image = tf.placeholder(dtype=tf.uint8, shape=img_shape_storage)\n",
    "        encoded_png = tf.image.encode_png(image)\n",
    "\n",
    "        num_images = len(images)\n",
    "        images = images.reshape(-1, img_w, img_h, 1)\n",
    "\n",
    "        with tf.Session('') as sess:\n",
    "            for j in range(num_images):\n",
    "                sys.stdout.write('\\r>> Converting image %d/%d' % (j + 1, num_images))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                png_string = sess.run(encoded_png, feed_dict={image: images[j]})\n",
    "                example = image_to_tfexample(png_string, 'png'.encode(), img_w, img_h, labels[j])\n",
    "                tfrecord_writer.write(example.SerializeToString())\n",
    "\n",
    "#                pngFname = target + \"/\" + \"train_%d.png\" % (j)\n",
    "#                pngFile=open(pngFname,'wb')\n",
    "#                pngFile.write(png_string);\n",
    "#                pngFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train Set 60000\n",
      "y_train_cls Set 60000\n",
      "x_test Set 10000\n",
      "y_test_cls Set 10000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "x_train = download_img(x_train_url, x_train_file, target,16)\n",
    "print(\"x_train Set\", len(x_train))\n",
    "\n",
    "y_train_cls = download_cls(y_train_url, y_train_file, target,8)\n",
    "print(\"y_train_cls Set\", len(y_train_cls))\n",
    "\n",
    "x_test = download_img(x_test_url, x_test_file, target,16)\n",
    "print(\"x_test Set\", len(x_test))\n",
    "\n",
    "y_test_cls = download_cls(y_test_url, y_test_file, target,8)\n",
    "print(\"y_test_cls Set\", len(y_test_cls))\n",
    "\n",
    "print(len(x_train))\n",
    "\n",
    "if not os.path.exists(training_recname):\n",
    "    with tf.python_io.TFRecordWriter(training_recname) as tfrecord_writer:\n",
    "        _add_to_tfrecord(x_train_rpath, y_train_rpath, 60000, tfrecord_writer, x_train, y_train_cls)\n",
    "\n",
    "if not os.path.exists(testing_recname):\n",
    "    with tf.python_io.TFRecordWriter(testing_recname) as tfrecord_writer:\n",
    "        _add_to_tfrecord(x_test_rpath, y_test_rpath, 10000, tfrecord_writer, x_test, y_test_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(record):\n",
    "    keys_to_features = {\n",
    "        \"image/encoded\":  tf.FixedLenFeature([], tf.string),\n",
    "        \"image/format\":  tf.FixedLenFeature([], tf.string),\n",
    "        \"image/class/label\": tf.FixedLenFeature([], tf.int64),\n",
    "        \"image/height\": tf.FixedLenFeature([], tf.int64),\n",
    "        \"image/width\": tf.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "\n",
    "    parsed = tf.parse_single_example(record, keys_to_features)\n",
    "\n",
    "    height = tf.cast(parsed[\"image/height\"], tf.int32)\n",
    "    width = tf.cast(parsed[\"image/width\"], tf.int32)\n",
    "    label = tf.cast(parsed[\"image/class/label\"], tf.int32)\n",
    "    image = tf.cast(tf.image.decode_png(parsed[\"image/encoded\"], channels=1), tf.float32)\n",
    "    \n",
    " \n",
    "    return {'image': image}, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(filenames):\n",
    "    \n",
    "    BATCH_SIZE = 128\n",
    "    THREADS = 4\n",
    "    PREFETCH = 64\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(filenames=filenames, num_parallel_reads=THREADS)\n",
    "    dataset = dataset.apply(\n",
    "        tf.contrib.data.shuffle_and_repeat(1024, 1)\n",
    "    )\n",
    "    dataset = dataset.apply(\n",
    "        tf.contrib.data.map_and_batch(parser, BATCH_SIZE)\n",
    "    )\n",
    "\n",
    "    dataset = dataset.prefetch(buffer_size=PREFETCH)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    return input_fn(filenames=[training_recname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_input_fn():\n",
    "    return input_fn(filenames=[testing_recname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode, params):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer\n",
    "    # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "    # MNIST images are 28x28 pixels, and have one color channel\n",
    "\n",
    "    net = features[\"image\"]\n",
    "\n",
    "    net = tf.identity(net, name=\"input_tensor\")\n",
    "\n",
    "    net = tf.reshape(net, [-1, 28, 28, 1])\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "    # Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    conv1 = tf.layers.conv2d(inputs=net, filters=32,kernel_size=[5, 5],padding=\"same\", activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    # First max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Convolutional Layer #2\n",
    "    # Computes 64 features using a 5x5 filter.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    conv2 = tf.layers.conv2d(inputs=pool1,filters=64,kernel_size=[5, 5],padding=\"same\",activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #2\n",
    "    # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Flatten tensor into a batch of vectors\n",
    "    # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "    # Dense Layer\n",
    "    # Densely connected layer with 1024 neurons\n",
    "    # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    # Output Tensor Shape: [batch_size, 1024]\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "    # Add dropout operation; 0.6 probability that element will be kept\n",
    "    dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits layer\n",
    "    # Input Tensor Shape: [batch_size, 1024]\n",
    "    # Output Tensor Shape: [batch_size, 10]\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1), \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(loss=loss,global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])\n",
    "    }\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '01_simple_MNIST', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2400138fd0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 29.91493, step = 0\n",
      "INFO:tensorflow:global_step/sec: 271.213\n",
      "INFO:tensorflow:loss = 0.82150996, step = 100 (0.369 sec)\n",
      "INFO:tensorflow:global_step/sec: 340.658\n",
      "INFO:tensorflow:loss = 0.39717412, step = 200 (0.294 sec)\n",
      "INFO:tensorflow:global_step/sec: 335.576\n",
      "INFO:tensorflow:loss = 0.42335713, step = 300 (0.298 sec)\n",
      "INFO:tensorflow:global_step/sec: 345.887\n",
      "INFO:tensorflow:loss = 0.33510008, step = 400 (0.289 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 469 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.13354482.\n",
      "Time Take per Iternation of training is : 186.365046%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-08-00:57:44\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-469\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-08-00:57:44\n",
      "INFO:tensorflow:Saving dict for global step 469: accuracy = 0.9555, global_step = 469, loss = 0.14135505\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 469: 01_simple_MNIST/model.ckpt-469\n",
      "Classification accuracy: 95.55%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-469\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 469 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.30679053, step = 469\n",
      "INFO:tensorflow:global_step/sec: 293.543\n",
      "INFO:tensorflow:loss = 0.3791888, step = 569 (0.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 345.034\n",
      "INFO:tensorflow:loss = 0.1811232, step = 669 (0.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 345.822\n",
      "INFO:tensorflow:loss = 0.22765328, step = 769 (0.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.297\n",
      "INFO:tensorflow:loss = 0.27337843, step = 869 (0.290 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 938 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.11859232.\n",
      "Time Take per Iternation of training is : 190.504236%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-08-00:57:46\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-938\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-08-00:57:47\n",
      "INFO:tensorflow:Saving dict for global step 938: accuracy = 0.9679, global_step = 938, loss = 0.10280247\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 938: 01_simple_MNIST/model.ckpt-938\n",
      "Classification accuracy: 96.79%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-938\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 938 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.16909751, step = 938\n",
      "INFO:tensorflow:global_step/sec: 292.902\n",
      "INFO:tensorflow:loss = 0.1624561, step = 1038 (0.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 345.523\n",
      "INFO:tensorflow:loss = 0.14615548, step = 1138 (0.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 346.302\n",
      "INFO:tensorflow:loss = 0.1516127, step = 1238 (0.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 341.343\n",
      "INFO:tensorflow:loss = 0.17712429, step = 1338 (0.293 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1407 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.16826564.\n",
      "Time Take per Iternation of training is : 182.352399%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-08-00:57:49\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-1407\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-08-00:57:49\n",
      "INFO:tensorflow:Saving dict for global step 1407: accuracy = 0.9742, global_step = 1407, loss = 0.083074175\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1407: 01_simple_MNIST/model.ckpt-1407\n",
      "Classification accuracy: 97.42%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-1407\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1407 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.1377203, step = 1407\n",
      "INFO:tensorflow:global_step/sec: 299.232\n",
      "INFO:tensorflow:loss = 0.1820637, step = 1507 (0.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 341.232\n",
      "INFO:tensorflow:loss = 0.09519609, step = 1607 (0.293 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.762\n",
      "INFO:tensorflow:loss = 0.16393664, step = 1707 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 346.219\n",
      "INFO:tensorflow:loss = 0.19190037, step = 1807 (0.289 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1876 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.051315714.\n",
      "Time Take per Iternation of training is : 186.128508%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-08-00:57:51\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-1876\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-08-00:57:51\n",
      "INFO:tensorflow:Saving dict for global step 1876: accuracy = 0.9769, global_step = 1876, loss = 0.07235754\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1876: 01_simple_MNIST/model.ckpt-1876\n",
      "Classification accuracy: 97.69%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-1876\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1876 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.07806479, step = 1876\n",
      "INFO:tensorflow:global_step/sec: 294.839\n",
      "INFO:tensorflow:loss = 0.04940208, step = 1976 (0.338 sec)\n",
      "INFO:tensorflow:global_step/sec: 348.082\n",
      "INFO:tensorflow:loss = 0.21309489, step = 2076 (0.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 346.469\n",
      "INFO:tensorflow:loss = 0.03642853, step = 2176 (0.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 342.165\n",
      "INFO:tensorflow:loss = 0.12308012, step = 2276 (0.292 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2345 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.09005699.\n",
      "Time Take per Iternation of training is : 189.172236%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2019-01-08-00:57:53\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-2345\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-08-00:57:53\n",
      "INFO:tensorflow:Saving dict for global step 2345: accuracy = 0.9785, global_step = 2345, loss = 0.06565974\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2345: 01_simple_MNIST/model.ckpt-2345\n",
      "Classification accuracy: 97.85%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-2345\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2345 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.09966725, step = 2345\n",
      "INFO:tensorflow:global_step/sec: 292.549\n",
      "INFO:tensorflow:loss = 0.075306475, step = 2445 (0.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 338.798\n",
      "INFO:tensorflow:loss = 0.09949135, step = 2545 (0.295 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.625\n",
      "INFO:tensorflow:loss = 0.13018794, step = 2645 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.693\n",
      "INFO:tensorflow:loss = 0.101622954, step = 2745 (0.291 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2814 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.04588386.\n",
      "Time Take per Iternation of training is : 185.345027%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-08-00:57:55\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-2814\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-08-00:57:56\n",
      "INFO:tensorflow:Saving dict for global step 2814: accuracy = 0.9803, global_step = 2814, loss = 0.059646185\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2814: 01_simple_MNIST/model.ckpt-2814\n",
      "Classification accuracy: 98.03%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-2814\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2814 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.043817993, step = 2814\n",
      "INFO:tensorflow:global_step/sec: 285.156\n",
      "INFO:tensorflow:loss = 0.11716355, step = 2914 (0.351 sec)\n",
      "INFO:tensorflow:global_step/sec: 339.637\n",
      "INFO:tensorflow:loss = 0.008861372, step = 3014 (0.294 sec)\n",
      "INFO:tensorflow:global_step/sec: 338.939\n",
      "INFO:tensorflow:loss = 0.03915163, step = 3114 (0.295 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.901\n",
      "INFO:tensorflow:loss = 0.124898545, step = 3214 (0.291 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3283 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.046627413.\n",
      "Time Take per Iternation of training is : 193.389686%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-08-00:57:58\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-3283\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-08-00:57:58\n",
      "INFO:tensorflow:Saving dict for global step 3283: accuracy = 0.981, global_step = 3283, loss = 0.05617667\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3283: 01_simple_MNIST/model.ckpt-3283\n",
      "Classification accuracy: 98.10%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-3283\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3283 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.055451795, step = 3283\n",
      "INFO:tensorflow:global_step/sec: 286.038\n",
      "INFO:tensorflow:loss = 0.1031917, step = 3383 (0.350 sec)\n",
      "INFO:tensorflow:global_step/sec: 342.552\n",
      "INFO:tensorflow:loss = 0.053309843, step = 3483 (0.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.741\n",
      "INFO:tensorflow:loss = 0.083681464, step = 3583 (0.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.728\n",
      "INFO:tensorflow:loss = 0.117469825, step = 3683 (0.291 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3752 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.012346305.\n",
      "Time Take per Iternation of training is : 185.307387%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-08-00:58:00\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-3752\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-08-00:58:00\n",
      "INFO:tensorflow:Saving dict for global step 3752: accuracy = 0.9817, global_step = 3752, loss = 0.053570088\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3752: 01_simple_MNIST/model.ckpt-3752\n",
      "Classification accuracy: 98.17%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-3752\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3752 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.07199426, step = 3752\n",
      "INFO:tensorflow:global_step/sec: 290.969\n",
      "INFO:tensorflow:loss = 0.048809282, step = 3852 (0.344 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.64\n",
      "INFO:tensorflow:loss = 0.024580091, step = 3952 (0.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.859\n",
      "INFO:tensorflow:loss = 0.0464598, step = 4052 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 345.982\n",
      "INFO:tensorflow:loss = 0.069827855, step = 4152 (0.289 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4221 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.023835978.\n",
      "Time Take per Iternation of training is : 187.576151%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-08-00:58:02\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-4221\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-08-00:58:02\n",
      "INFO:tensorflow:Saving dict for global step 4221: accuracy = 0.9833, global_step = 4221, loss = 0.050605223\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4221: 01_simple_MNIST/model.ckpt-4221\n",
      "Classification accuracy: 98.33%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-4221\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 4221 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.023268688, step = 4221\n",
      "INFO:tensorflow:global_step/sec: 286.724\n",
      "INFO:tensorflow:loss = 0.044648018, step = 4321 (0.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 346.052\n",
      "INFO:tensorflow:loss = 0.11778846, step = 4421 (0.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.506\n",
      "INFO:tensorflow:loss = 0.110520944, step = 4521 (0.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 334.451\n",
      "INFO:tensorflow:loss = 0.11274671, step = 4621 (0.299 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4690 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.11932238.\n",
      "Time Take per Iternation of training is : 189.954260%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2019-01-08-00:58:04\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-4690\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-08-00:58:05\n",
      "INFO:tensorflow:Saving dict for global step 4690: accuracy = 0.9843, global_step = 4690, loss = 0.047141116\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4690: 01_simple_MNIST/model.ckpt-4690\n",
      "Classification accuracy: 98.43%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-4690\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 4690 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.10184384, step = 4690\n",
      "INFO:tensorflow:global_step/sec: 289.302\n",
      "INFO:tensorflow:loss = 0.12126458, step = 4790 (0.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.058\n",
      "INFO:tensorflow:loss = 0.02131297, step = 4890 (0.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 342.888\n",
      "INFO:tensorflow:loss = 0.019510923, step = 4990 (0.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.016\n",
      "INFO:tensorflow:loss = 0.040932104, step = 5090 (0.291 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5159 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.010699339.\n",
      "Time Take per Iternation of training is : 185.783041%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-08-00:58:07\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-5159\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-08-00:58:07\n",
      "INFO:tensorflow:Saving dict for global step 5159: accuracy = 0.9841, global_step = 5159, loss = 0.047450464\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5159: 01_simple_MNIST/model.ckpt-5159\n",
      "Classification accuracy: 98.41%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-5159\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 5159 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.029984105, step = 5159\n",
      "INFO:tensorflow:global_step/sec: 289.59\n",
      "INFO:tensorflow:loss = 0.02846096, step = 5259 (0.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 345.967\n",
      "INFO:tensorflow:loss = 0.02332128, step = 5359 (0.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 341.6\n",
      "INFO:tensorflow:loss = 0.0104673505, step = 5459 (0.293 sec)\n",
      "INFO:tensorflow:global_step/sec: 335.37\n",
      "INFO:tensorflow:loss = 0.03525894, step = 5559 (0.298 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5628 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.02885156.\n",
      "Time Take per Iternation of training is : 184.494466%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-08-00:58:09\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-5628\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-08-00:58:09\n",
      "INFO:tensorflow:Saving dict for global step 5628: accuracy = 0.9851, global_step = 5628, loss = 0.043090638\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5628: 01_simple_MNIST/model.ckpt-5628\n",
      "Classification accuracy: 98.51%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-5628\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 5628 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.03907161, step = 5628\n",
      "INFO:tensorflow:global_step/sec: 294.823\n",
      "INFO:tensorflow:loss = 0.0795244, step = 5728 (0.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.067\n",
      "INFO:tensorflow:loss = 0.0635212, step = 5828 (0.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.135\n",
      "INFO:tensorflow:loss = 0.07076931, step = 5928 (0.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 348.74\n",
      "INFO:tensorflow:loss = 0.07538611, step = 6028 (0.287 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6097 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.01973099.\n",
      "Time Take per Iternation of training is : 191.715772%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-08-00:58:11\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-6097\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-08-00:58:11\n",
      "INFO:tensorflow:Saving dict for global step 6097: accuracy = 0.9853, global_step = 6097, loss = 0.04177852\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6097: 01_simple_MNIST/model.ckpt-6097\n",
      "Classification accuracy: 98.53%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-6097\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 6097 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.15888348, step = 6097\n",
      "INFO:tensorflow:global_step/sec: 287.451\n",
      "INFO:tensorflow:loss = 0.0322032, step = 6197 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.214\n",
      "INFO:tensorflow:loss = 0.06356961, step = 6297 (0.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.456\n",
      "INFO:tensorflow:loss = 0.022071555, step = 6397 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 341.755\n",
      "INFO:tensorflow:loss = 0.018016394, step = 6497 (0.293 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6566 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.040461782.\n",
      "Time Take per Iternation of training is : 184.389397%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-08-00:58:13\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-6566\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-08-00:58:14\n",
      "INFO:tensorflow:Saving dict for global step 6566: accuracy = 0.9862, global_step = 6566, loss = 0.041504484\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6566: 01_simple_MNIST/model.ckpt-6566\n",
      "Classification accuracy: 98.62%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-6566\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 6566 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.07327689, step = 6566\n",
      "INFO:tensorflow:global_step/sec: 288.086\n",
      "INFO:tensorflow:loss = 0.036271952, step = 6666 (0.347 sec)\n",
      "INFO:tensorflow:global_step/sec: 337.978\n",
      "INFO:tensorflow:loss = 0.027608199, step = 6766 (0.296 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.487\n",
      "INFO:tensorflow:loss = 0.04688899, step = 6866 (0.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 338.199\n",
      "INFO:tensorflow:loss = 0.05166704, step = 6966 (0.296 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7035 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.008438403.\n",
      "Time Take per Iternation of training is : 192.664007%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2019-01-08-00:58:16\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-7035\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-08-00:58:16\n",
      "INFO:tensorflow:Saving dict for global step 7035: accuracy = 0.9863, global_step = 7035, loss = 0.040243667\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 7035: 01_simple_MNIST/model.ckpt-7035\n",
      "Classification accuracy: 98.63%"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "model = tf.estimator.Estimator(model_fn=cnn_model_fn,\n",
    "                               params={\"learning_rate\": 1e-4},\n",
    "                               model_dir=\"01_simple_MNIST\")\n",
    "\n",
    "import timeit\n",
    "\n",
    "EPOCHS = 15\n",
    "STEP_SIZE = 500\n",
    "count = 0\n",
    "\n",
    "while (count < EPOCHS):\n",
    "    start_time = timeit.default_timer()\n",
    "    model.train(input_fn=train_input_fn, steps=STEP_SIZE)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    sys.stdout.write(\"Time Take per Iternation of training is : {0:%}\".format(elapsed))\n",
    "\n",
    "    result = model.evaluate(input_fn=val_input_fn)\n",
    "    #print(result)\n",
    "    sys.stdout.write(\"Classification accuracy: {0:.2%}\".format(result[\"accuracy\"]))\n",
    "    sys.stdout.flush()\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-7035\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./01_simple_MNIST/export/temp-b'1546908979'/saved_model.pbtxt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'./01_simple_MNIST/export/1546908979'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_serving_input_receiver_fn():\n",
    "    inputs = {'image': tf.placeholder(shape=[28,28,1], dtype=tf.float32, name='image')}\n",
    "    return tf.estimator.export.build_raw_serving_input_receiver_fn(inputs)\n",
    "\n",
    "export_dir = os.path.join('./01_simple_MNIST/', 'export')\n",
    "\n",
    "if tf.gfile.Exists(export_dir):\n",
    "        tf.gfile.DeleteRecursively(export_dir)\n",
    "\n",
    "#model.export_savedmodel(export_dir_base=export_dir,serving_input_receiver_fn=make_serving_input_receiver_fn(),as_text=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./01_simple_MNIST/export/1546908979/variables/variables\n"
     ]
    }
   ],
   "source": [
    "export_dir = os.path.join('./01_simple_MNIST/', 'export')\n",
    "saved_model_dir = os.path.join(export_dir, os.listdir(export_dir)[-1]) \n",
    "\n",
    "predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "  export_dir = saved_model_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Result: Pred. No. : 4')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAEaFJREFUeJzt3XuMXPV5xvHvgwOkXGPkBRxjswHcNqQCg7YQCRJcJYSLgszNKZCCi4CNVKiSltCA44CbC3WpgaBCg0wxtkkMRSJckkKKuTRAL64XZIMBubHQYgzGXssgr4lTMH77x5wlk2XnzOzczqx/z0da7cx5z+XdsZ8515mjiMDM0rNb0Q2YWTEcfrNEOfxmiXL4zRLl8JslyuE3S5TDP0ZImi5pfdF9VCJprqQfF92H1c7hr4OkfknbJW2T9JakRZL2KaCHL45i/OmSdmY9D0paI+niVvaY08tcSSFpZtmwj2XDutvYx1RJv0n1Tcvhr98ZEbEPMA04Brim4H5q8WbW837At4A7JB05fCRJH2tDL1uA70oa14ZlVXIbsKLA5RfK4W9QRLwF/BulNwEAJO0pab6kdZI2Srpd0u9ltQmSfi7pHUlbJD0jabesFpKOKJvPIknfH75MSXcDU4CfZWvyvxllzxERDwJvA0dK6s6WfYmkdcCT2XI+K+k/s15XSZpe1sOnJP0y24pYBkwYTQ/AL4D3gD8bqShpf0lLJA1Iek3SnKHXqRkknQe8AzzRrHmONQ5/gyQdApwGrC0b/PfA71N6QzgCmARcm9WuBNYDXcBBwGxgVNdYR8SFwDqyrY+IuCHr5QVJF9TQ826SzgI+AbxYVjoJ+DRwiqRJwL8C3wcOAL4J3C+pKxt3KfAcpdB/D5g1mr+B0t/8HeA6SbuPUP9HYH/gsKyvi4CadlMkXS3p5zn1/YDvUvq3SJbDX78HJQ0CrwObgOsAJAm4DPiriNgSEYPA9cB52XTvAxOBQyPi/Yh4Jpr0AYuIOCoiluaM8klJ7wCbs34vjIg1ZfW5EfFuRGyntEZ+JCIeiYidEbEM6ANOlzQF+GPgOxHxfxHxNPCzOvp9GBgALi0fnu0K/ClwTUQMRkQ/cCNwYY3znRcRX84Z5XvAnRHx+mh73pU4/PU7MyL2BaYDf8hvN3u7gL2A57LN5XcobeIOrTH/gdJWwmOSXpV0dRt7fjMiPhERB0TEtIi4d1i9PAyHAjOH/obs7ziR0hvXJ4G3I+LdsvFfq7OnOcC3gY+XDZsA7DFsnq9R2oJqiKRpwBeBmxud11jXjgM7u7SI+KWkRcB84ExKa9XtwGci4o0Rxh+ktLl5paTPAE9JWhERTwC/pvTGMeRgSrsIIy66eX/FiPN8Hbg7Ii4bPpKkQ4HxkvYuewOYUk9PEbFM0lrgL8oGb6a0hXQo8HLZ/D/yetZhOtANrCttpLEPME7SkRFxbBPmP2Z4zd8cPwROljQtInYCdwA3SzoQQNIkSadkj78s6Yhs92Ar8EH2A7ASuEDSOEmnUtrXrWQjpf3hVvkxcIakU7J+Pp6dLjwkIl6jtAvwt5L2kHQicEYDy/o28OFBy4j4ALgP+IGkfbM3m7/OemrUAuBwSsdjpgG3Uzq2cUoT5j2mOPxNEBEDwBJKB7CgdBptLfDfkrYCjwN/kNWmZs+3Af8F/FNE/HtW+zqlEL0DfBV4MGexfwfMyTbJvwkg6SVJX23S3/Q6MIPSAckBSlsCV/Hb/zMXAMdTOmV3HaW//0PZWYjP1bis/wD+Z9jgvwTeBV4FnqV0gHFhNu/PSdpWaX6SZkt6tMKyfh0Rbw39UPp3+E32b5gU+cs8zNLkNb9Zohx+s0Q5/GaJcvjNEtXW8/wTJkyI7u7udi7SLCn9/f1s3rxZtYzbUPizc9G3AOOAf46IeXnjd3d309fX18gizSxHT09PzePWvdmfXX99G6UPtRwJnD/Sx0PNrDM1ss9/HLA2Il6NiPeAeyldFGJmY0Aj4Z/E734QZD0jfPBCUq+kPkl9AwPJXURl1rEaCf9IBxU+crlgRCyIiJ6I6Onq6hphEjMrQiPhXw9MLnt+CPBmY+2YWbs0Ev4VwNTs65z2oPRlFQ83py0za7W6T/VFxA5JV1D6/rpxwMKIeKlpnZlZSzV0nj8iHgEeaVIvZtZGvrzXLFEOv1miHH6zRDn8Zoly+M0S5fCbJcrhN0uUw2+WKIffLFEOv1miHH6zRDn8Zoly+M0S5fCbJcrhN0uUw2+WKIffLFEOv1miHH6zRDn8Zoly+M0S5fCbJcrhN0uUw2+WKIffLFEOv1miHH6zRDn8Zoly+M0S5fCbJaqhW3RL6gcGgQ+AHRHR04ymzKz1Ggp/5k8iYnMT5mNmbeTNfrNENRr+AB6T9Jyk3pFGkNQrqU9S38DAQIOLM7NmaTT8J0TEscBpwOWSPj98hIhYEBE9EdHT1dXV4OLMrFkaCn9EvJn93gQ8ABzXjKbMrPXqDr+kvSXtO/QY+BKwulmNmVlrNXK0/yDgAUlD81kaEb9oSlfWNNWOs9x000259eXLl+fWn3zyyVH3NOScc87JrVfbTbz99tvrXrY1EP6IeBU4uom9mFkb+VSfWaIcfrNEOfxmiXL4zRLl8Jslqhkf7LEOdsMNN+TWFy9enFu/7bbbGlr+4OBgxdrjjz+eO+1FF13U0LItn9f8Zoly+M0S5fCbJcrhN0uUw2+WKIffLFEOv1mifJ5/F9DX11extmjRotxpJ0+enFufOXNmPS19aP369RVrW7duzZ12/PjxDS3b8nnNb5Yoh98sUQ6/WaIcfrNEOfxmiXL4zRLl8Jslyuf5dwFLly6tWNu8Of8eqlOnTm12O7/j+uuvb+n8rX5e85slyuE3S5TDb5Yoh98sUQ6/WaIcfrNEOfxmifJ5/jHgnnvuya0/9NBDdc97yZIldU9bixUrVtQ97cknn9zETmy4qmt+SQslbZK0umzYAZKWSfpV9tvfumA2xtSy2b8IOHXYsKuBJyJiKvBE9tzMxpCq4Y+Ip4EtwwbPAIbu87QYOLPJfZlZi9V7wO+giNgAkP0+sNKIknol9UnqGxgYqHNxZtZsLT/aHxELIqInInq6urpavTgzq1G94d8oaSJA9ntT81oys3aoN/wPA7Oyx7OA+s81mVkhqp7nl3QPMB2YIGk9cB0wD7hP0iXAOqCxL3dPXN532wPMnj07t97f31+xdvHFF+dOe/jhh+fWW+mwww7LrR911FFt6iRNVcMfEedXKH2hyb2YWRv58l6zRDn8Zoly+M0S5fCbJcrhN0uUP9LbAWbMmJFbzzuVB9Db21uxNm/evNxpJeXWq6l2yXbebbjPPffc3Gn333//unqy2njNb5Yoh98sUQ6/WaIcfrNEOfxmiXL4zRLl8Jslyuf522DdunW59bVr1zY0/+XLl1esdXd3505b7RqDal8L/v777+fWt2/fXve8zznnnNz6SSedlFu3fF7zmyXK4TdLlMNvliiH3yxRDr9Zohx+s0Q5/GaJ8nn+NpgyZUpu/eijj86tP/PMM7n1VatWjbqnIXfffXfd0zZq/vz5uXWfx28tr/nNEuXwmyXK4TdLlMNvliiH3yxRDr9Zohx+s0T5PH8HeOqpp3LrO3fubFMnH7Vjx47c+l577ZVb32+//SrWqn1e31qr6ppf0kJJmyStLhs2V9IbklZmP6e3tk0za7ZaNvsXAaeOMPzmiJiW/TzS3LbMrNWqhj8inga2tKEXM2ujRg74XSHphWy3YHylkST1SuqT1Fftvm5m1j71hv9HwOHANGADcGOlESNiQUT0RERPV1dXnYszs2arK/wRsTEiPoiIncAdwHHNbcvMWq2u8EuaWPb0LGB1pXHNrDNVPc8v6R5gOjBB0nrgOmC6pGlAAP3A11rY4y5v3LhxDdVb6eWXX25o+rPPPrtibbfdfI1ZkaqGPyLOH2HwnS3oxczayG+9Zoly+M0S5fCbJcrhN0uUw2+WKH+kN3Hbtm3LrV9zzTUNzT/vVJ8Vy2t+s0Q5/GaJcvjNEuXwmyXK4TdLlMNvliiH3yxRPs+fuDVr1uTWH3300dz6pEmTcuvHH3/8qHuy9vCa3yxRDr9Zohx+s0Q5/GaJcvjNEuXwmyXK4TdLlM/zJ27x4sUNTX/ttdfm1g888MCG5m+t4zW/WaIcfrNEOfxmiXL4zRLl8JslyuE3S5TDb5aoWm7RPRlYAhwM7AQWRMQtkg4A/gXopnSb7q9ExNuta9XqsWrVqtz6rbfemlvffffdc+u9vb2j7sk6Qy1r/h3AlRHxaeCzwOWSjgSuBp6IiKnAE9lzMxsjqoY/IjZExPPZ40HgFWASMAMYujxsMXBmq5o0s+Yb1T6/pG7gGGA5cFBEbIDSGwTg6zjNxpCawy9pH+B+4BsRsXUU0/VK6pPUNzAwUE+PZtYCNYVf0u6Ugv+TiPhpNnijpIlZfSKwaaRpI2JBRPRERE9XV1czejazJqgafkkC7gReiYibykoPA7Oyx7OAh5rfnpm1Si0f6T0BuBB4UdLKbNhsYB5wn6RLgHXAzNa0aNUMDg5WrF111VW500ZEbt2n8nZdVcMfEc8CqlD+QnPbMbN28RV+Zoly+M0S5fCbJcrhN0uUw2+WKIffLFH+6u5dwAMPPFCxtmzZstxpJ06cmFufM2dOXT1Z5/Oa3yxRDr9Zohx+s0Q5/GaJcvjNEuXwmyXK4TdLlM/z7wJWrlxZfaQKLr300tz6wQcfXPe8rbN5zW+WKIffLFEOv1miHH6zRDn8Zoly+M0S5fCbJcrn+XcBd911V93Tzp49u4md2FjiNb9Zohx+s0Q5/GaJcvjNEuXwmyXK4TdLlMNvlqiq5/klTQaWAAcDO4EFEXGLpLnAZcBANursiHikVY1aZWeccUbF2pYtW3Kn3XPPPZvdjo0RtVzkswO4MiKel7Qv8JykoTtB3BwR81vXnpm1StXwR8QGYEP2eFDSK8CkVjdmZq01qn1+Sd3AMcDybNAVkl6QtFDS+ArT9Erqk9Q3MDAw0ihmVoCawy9pH+B+4BsRsRX4EXA4MI3SlsGNI00XEQsioicierq6uprQspk1Q03hl7Q7peD/JCJ+ChARGyPig4jYCdwBHNe6Ns2s2aqGX5KAO4FXIuKmsuHlt3c9C1jd/PbMrFVqOdp/AnAh8KKkoe+Ing2cL2kaEEA/8LWWdGhVLVmypOgWbAyq5Wj/s4BGKPmcvtkY5iv8zBLl8JslyuE3S5TDb5Yoh98sUQ6/WaIcfrNEOfxmiXL4zRLl8JslyuE3S5TDb5Yoh98sUQ6/WaIUEe1bmDQAvFY2aAKwuW0NjE6n9tapfYF7q1czezs0Imr6vry2hv8jC5f6IqKnsAZydGpvndoXuLd6FdWbN/vNEuXwmyWq6PAvKHj5eTq1t07tC9xbvQrprdB9fjMrTtFrfjMriMNvlqhCwi/pVElrJK2VdHURPVQiqV/Si5JWSuoruJeFkjZJWl027ABJyyT9Kvs94j0SC+ptrqQ3stdupaTTC+ptsqSnJL0i6SVJX8+GF/ra5fRVyOvW9n1+SeOA/wVOBtYDK4DzI+LltjZSgaR+oCciCr8gRNLngW3Akoj4o2zYDcCWiJiXvXGOj4hvdUhvc4FtRd+2Pbub1MTy28oDZwJ/ToGvXU5fX6GA162INf9xwNqIeDUi3gPuBWYU0EfHi4ingS3DBs8AFmePF1P6z9N2FXrrCBGxISKezx4PAkO3lS/0tcvpqxBFhH8S8HrZ8/UU+AKMIIDHJD0nqbfoZkZwUERsgNJ/JuDAgvsZrupt29tp2G3lO+a1q+d2981WRPhHuvVXJ51vPCEijgVOAy7PNm+tNjXdtr1dRritfEeo93b3zVZE+NcDk8ueHwK8WUAfI4qIN7Pfm4AH6Lxbj28cukNy9ntTwf18qJNu2z7SbeXpgNeuk253X0T4VwBTJX1K0h7AecDDBfTxEZL2zg7EIGlv4Et03q3HHwZmZY9nAQ8V2Mvv6JTbtle6rTwFv3addrv7Qq7wy05l/BAYByyMiB+0vYkRSDqM0toeSncwXlpkb5LuAaZT+sjnRuA64EHgPmAKsA6YGRFtP/BWobfplDZdP7xt+9A+dpt7OxF4BngR2JkNnk1p/7qw1y6nr/Mp4HXz5b1mifIVfmaJcvjNEuXwmyXK4TdLlMNvliiH3yxRDr9Zov4fPxIy53D1jKsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy\n",
    "import random\n",
    "\n",
    "dir = target + \"/PNGs/\"\n",
    "pngFname = dir + random.choice(os.listdir(dir))\n",
    "\n",
    "pngFile=open(pngFname,'rb')\n",
    "png_string = pngFile.read();\n",
    "pngFile.close()\n",
    "\n",
    "#sample = tf.cast(tf.image.decode_png(png_string, channels=1), tf.int8)\n",
    "sample = tf.cast(tf.image.decode_png(png_string, channels=1), tf.float32)\n",
    "\n",
    "a = np.ones(shape=(28,28,1),dtype=np.float32)    \n",
    "\n",
    "img = tf.reshape(sample, [28, 28, 1])\n",
    "\n",
    "with sess.as_default():\n",
    "    a = img.eval()\n",
    "\n",
    "output = predictor_fn({'image': a})\n",
    "#print(output)\n",
    "#print (\"train_data.shape: \" + str(sample.get_shape()))\n",
    "#print (\"train_data.shape: \" + str(img.get_shape()))\n",
    "\n",
    "max = 0\n",
    "\n",
    "for x in range(10):\n",
    "    if output['probabilities'][0][x] >= output['probabilities'][0][max]:\n",
    "        max = x\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.imshow(a.reshape(img_shape), cmap='binary')\n",
    "xlabel = \"Result: Pred. No. : {0}\".format(max)\n",
    "ax.set_title(xlabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
