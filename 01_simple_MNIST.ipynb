{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 01: Writing first MNIST Program\n",
    "By Rahul GAWAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This program is inspired by [GitHub](https://github.com/Hvass-Labs/TensorFlow-Tutorials)\n",
    "\n",
    "Python used: 3.6\n",
    "TensorFlow version: 1.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import urllib.request\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_w = 28\n",
    "img_h = 28\n",
    "img_shape = (img_w, img_h)\n",
    "img_shape_storage = (img_w, img_h, 1)\n",
    "\n",
    "url = \"https://storage.googleapis.com/cvdf-datasets/mnist/\"\n",
    "\n",
    "x_train_file = \"train-images-idx3-ubyte.gz\"\n",
    "y_train_file = \"train-labels-idx1-ubyte.gz\"\n",
    "x_test_file = \"t10k-images-idx3-ubyte.gz\"\n",
    "y_test_file = \"t10k-labels-idx1-ubyte.gz\"\n",
    "\n",
    "target = \"mnist_dataset\"\n",
    "\n",
    "x_train_url = url + x_train_file\n",
    "y_train_url = url + y_train_file\n",
    "x_test_url = url + x_test_file\n",
    "y_test_url = url + y_test_file\n",
    "\n",
    "x_train_rpath = target + \"/\" + x_train_file\n",
    "y_train_rpath = target + \"/\" + y_train_file\n",
    "x_test_rpath = target + \"/\" + x_test_file\n",
    "y_test_rpath = target + \"/\" + y_test_file\n",
    "\n",
    "training_recname = '%s/mnist_%s.tfrecord' % (target, x_train_file)\n",
    "testing_recname = '%s/mnist_%s.tfrecord' % (target, x_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset to a local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print_download_progress(count, block_size, total_size):\n",
    "    \"\"\"\n",
    "    Function used for printing the download progress.\n",
    "    Used as a call-back function in maybe_download_and_extract().\n",
    "    \"\"\"\n",
    "\n",
    "    # Percentage completion.\n",
    "    pct_complete = float(count * block_size) / total_size\n",
    "\n",
    "    # Limit it because rounding errors may cause it to exceed 100%.\n",
    "    pct_complete = min(1.0, pct_complete)\n",
    "\n",
    "    # Status-message. Note the \\r which means the line should overwrite itself.\n",
    "    msg = \"\\r- Download progress: {0:.1%}\".format(pct_complete)\n",
    "\n",
    "    # Print it.\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_img(base_url, filename, download_dir, offset):\n",
    "\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "\n",
    "    file_path = os.path.join(download_dir, filename)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"\\nDownloading\", base_url, \"at\", download_dir, \".\")\n",
    "        file_path, _ = urllib.request.urlretrieve( base_url, file_path, _print_download_progress)\n",
    "\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=offset)\n",
    "\n",
    "    img_size = 28\n",
    "    img_size_flat = img_size * img_size\n",
    "    num_channels = 1\n",
    "    img_shape_full = (img_size, img_size, num_channels)\n",
    "    images_flat = data.reshape(-1, img_size_flat)\n",
    "    return images_flat\n",
    "\n",
    "def download_cls(base_url, filename, download_dir,  offset):\n",
    "\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "\n",
    "    file_path = os.path.join(download_dir, filename)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"\\nDownloading\", base_url, \"at\", download_dir, \".\")\n",
    "        file_path, _ = urllib.request.urlretrieve( base_url, file_path, _print_download_progress)\n",
    "\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8,  offset=offset)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x_train Set 60000\n",
      "y_train_cls Set 60000\n",
      "x_test Set 10000\n",
      "y_test_cls Set 10000\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "\n",
    "#save_path = os.path.join(download_dir, filename)\n",
    "\n",
    "x_train = download_img(x_train_url, x_train_file, target,16)\n",
    "print(\"x_train Set\", len(x_train))\n",
    "\n",
    "y_train_cls = download_cls(y_train_url, y_train_file, target,8)\n",
    "print(\"y_train_cls Set\", len(y_train_cls))\n",
    "\n",
    "x_test = download_img(x_test_url, x_test_file, target,16)\n",
    "print(\"x_test Set\", len(x_test))\n",
    "\n",
    "y_test_cls = download_cls(y_test_url, y_test_file, target,8)\n",
    "print(\"y_test_cls Set\", len(y_test_cls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set 55000  images and  55000  classes.\n",
      "Validation Set 5000  images and  5000  classes.\n",
      "Test Set 10000  images and  10000  classes.\n"
     ]
    }
   ],
   "source": [
    "num_train = 55000\n",
    "num_val = 5000\n",
    "num_test = 10000\n",
    "\n",
    "X_train = x_train[0:num_train] / 255.0\n",
    "Y_train_cls = y_train_cls[0:num_train]\n",
    "\n",
    "X_val = x_train[num_train:] / 255.0\n",
    "Y_val_cls = y_train_cls[num_train:]\n",
    "\n",
    "print(\"Training Set\", len(X_train), \" images and \", len(Y_train_cls), \" classes.\")\n",
    "print(\"Validation Set\", len(X_val), \" images and \", len(Y_val_cls), \" classes.\")\n",
    "\n",
    "X_test = x_train[0:num_test] / 255.0\n",
    "Y_test_cls = y_test_cls[0:num_test]\n",
    "\n",
    "print(\"Test Set\", len(X_test), \" images and \", len(Y_test_cls), \" classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None, img_w=28, img_h=28):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "\n",
    "\n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the first images from the test-set.\n",
    "images = X_test[0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = Y_test_cls[0:9]\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images=images, cls_true=cls_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_cls_char = Y_train_cls.astype(np.int)\n",
    "y_val_cls_char = Y_val_cls.astype(np.int)\n",
    "y_test_cls_char = Y_test_cls.astype(np.int)\n",
    "\n",
    "Y_train_OHE = np.eye(10, dtype=float)[y_train_cls_char]\n",
    "Y_val_OHE = np.eye(10, dtype=float)[y_val_cls_char]\n",
    "Y_test_OHE = np.eye(10, dtype=float)[y_test_cls_char]\n",
    "\n",
    "Y_test_OHE[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-record (Optional)\n",
    "\n",
    "[Referred this implementation to create tf.record](https://github.com/tensorflow/models/blob/master/research/slim/datasets/download_and_convert_mnist.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int64_feature(values):\n",
    "  if not isinstance(values, (tuple, list)):\n",
    "    values = [values]\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tfexample(image_data, image_format, height, width, class_id):\n",
    "  return tf.train.Example(features=tf.train.Features(feature={\n",
    "      'image/encoded':  tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_data])),\n",
    "      'image/format':  tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_format])),\n",
    "      'image/class/label': int64_feature(class_id),\n",
    "      'image/height': int64_feature(height),\n",
    "      'image/width': int64_feature(width)\n",
    "  }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _add_to_tfrecord(data_filename, labels_filename, num_images, tfrecord_writer, images, labels):\n",
    "    with tf.Graph().as_default():\n",
    "        image = tf.placeholder(dtype=tf.uint8, shape=img_shape_storage)\n",
    "        encoded_png = tf.image.encode_png(image)\n",
    "\n",
    "        num_images = len(images)\n",
    "        images = images.reshape(-1, img_w, img_h, 1)\n",
    "\n",
    "        with tf.Session('') as sess:\n",
    "            for j in range(num_images):\n",
    "                sys.stdout.write('\\r>> Converting image %d/%d' % (j + 1, num_images))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                png_string = sess.run(encoded_png, feed_dict={image: images[j]})\n",
    "                example = image_to_tfexample(png_string, 'png'.encode(), img_w, img_h, labels[j])\n",
    "                tfrecord_writer.write(example.SerializeToString())\n",
    "\n",
    "#                pngFname = target + \"/\" + \"train_%d.png\" % (j)\n",
    "#                pngFile=open(pngFname,'wb')\n",
    "#                pngFile.write(png_string);\n",
    "#                pngFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train Set 60000\n",
      "y_train_cls Set 60000\n",
      "x_test Set 10000\n",
      "y_test_cls Set 10000\n",
      "60000\n",
      ">> Converting image 3000/10000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Converting image 8398/10000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train = download_img(x_train_url, x_train_file, target,16)\n",
    "print(\"x_train Set\", len(x_train))\n",
    "\n",
    "y_train_cls = download_cls(y_train_url, y_train_file, target,8)\n",
    "print(\"y_train_cls Set\", len(y_train_cls))\n",
    "\n",
    "x_test = download_img(x_test_url, x_test_file, target,16)\n",
    "print(\"x_test Set\", len(x_test))\n",
    "\n",
    "y_test_cls = download_cls(y_test_url, y_test_file, target,8)\n",
    "print(\"y_test_cls Set\", len(y_test_cls))\n",
    "\n",
    "print(len(x_train))\n",
    "\n",
    "if not os.path.exists(training_recname):\n",
    "    with tf.python_io.TFRecordWriter(training_recname) as tfrecord_writer:\n",
    "        _add_to_tfrecord(x_train_rpath, y_train_rpath, 60000, tfrecord_writer, x_train, y_train_cls)\n",
    "\n",
    "if not os.path.exists(testing_recname):\n",
    "    with tf.python_io.TFRecordWriter(testing_recname) as tfrecord_writer:\n",
    "        _add_to_tfrecord(x_test_rpath, y_test_rpath, 10000, tfrecord_writer, x_test, y_test_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(record):\n",
    "    keys_to_features = {\n",
    "        \"image/encoded\":  tf.FixedLenFeature([], tf.string),\n",
    "        \"image/format\":  tf.FixedLenFeature([], tf.string),\n",
    "        \"image/class/label\": tf.FixedLenFeature([], tf.int64),\n",
    "        \"image/height\": tf.FixedLenFeature([], tf.int64),\n",
    "        \"image/width\": tf.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "\n",
    "    parsed = tf.parse_single_example(record, keys_to_features)\n",
    "\n",
    "    height = tf.cast(parsed[\"image/height\"], tf.int32)\n",
    "    width = tf.cast(parsed[\"image/width\"], tf.int32)\n",
    "    label = tf.cast(parsed[\"image/class/label\"], tf.int32)\n",
    "    image = tf.cast(tf.image.decode_png(parsed[\"image/encoded\"], channels=1), tf.float32)\n",
    "    \n",
    " \n",
    "    return {'image': image}, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(filenames):\n",
    "    \n",
    "    BATCH_SIZE = 128\n",
    "    THREADS = 4\n",
    "    PREFETCH = 64\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(filenames=filenames, num_parallel_reads=THREADS)\n",
    "    dataset = dataset.apply(\n",
    "        tf.contrib.data.shuffle_and_repeat(1024, 1)\n",
    "    )\n",
    "    dataset = dataset.apply(\n",
    "        tf.contrib.data.map_and_batch(parser, BATCH_SIZE)\n",
    "    )\n",
    "\n",
    "    dataset = dataset.prefetch(buffer_size=PREFETCH)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    return input_fn(filenames=[training_recname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_input_fn():\n",
    "    return input_fn(filenames=[testing_recname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode, params):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer\n",
    "    # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "    # MNIST images are 28x28 pixels, and have one color channel\n",
    "\n",
    "    net = features[\"image\"]\n",
    "\n",
    "    net = tf.identity(net, name=\"input_tensor\")\n",
    "\n",
    "    net = tf.reshape(net, [-1, 28, 28, 1])\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "    # Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    conv1 = tf.layers.conv2d(inputs=net, filters=32,kernel_size=[5, 5],padding=\"same\", activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    # First max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Convolutional Layer #2\n",
    "    # Computes 64 features using a 5x5 filter.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    conv2 = tf.layers.conv2d(inputs=pool1,filters=64,kernel_size=[5, 5],padding=\"same\",activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #2\n",
    "    # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Flatten tensor into a batch of vectors\n",
    "    # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "    # Dense Layer\n",
    "    # Densely connected layer with 1024 neurons\n",
    "    # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    # Output Tensor Shape: [batch_size, 1024]\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "    # Add dropout operation; 0.6 probability that element will be kept\n",
    "    dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits layer\n",
    "    # Input Tensor Shape: [batch_size, 1024]\n",
    "    # Output Tensor Shape: [batch_size, 10]\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1), \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(loss=loss,global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])\n",
    "    }\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'model5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5ea9368b70>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into model5/model.ckpt.\n",
      "INFO:tensorflow:loss = 52.114265, step = 0\n",
      "INFO:tensorflow:global_step/sec: 317.604\n",
      "INFO:tensorflow:loss = 0.7899664, step = 100 (0.315 sec)\n",
      "INFO:tensorflow:global_step/sec: 372.092\n",
      "INFO:tensorflow:loss = 0.4037754, step = 200 (0.269 sec)\n",
      "INFO:tensorflow:global_step/sec: 377.361\n",
      "INFO:tensorflow:loss = 0.39957854, step = 300 (0.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 375.742\n",
      "INFO:tensorflow:loss = 0.23485638, step = 400 (0.266 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 469 into model5/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.19282524.\n",
      "Time Take per Iternation of training is : 509.518043%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-02-03:43:01\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-469\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-02-03:43:01\n",
      "INFO:tensorflow:Saving dict for global step 469: accuracy = 0.9508, global_step = 469, loss = 0.15955925\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 469: model5/model.ckpt-469\n",
      "Classification accuracy: 95.08%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-469\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 469 into model5/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.34318507, step = 469\n",
      "INFO:tensorflow:global_step/sec: 319.048\n",
      "INFO:tensorflow:loss = 0.1780282, step = 569 (0.314 sec)\n",
      "INFO:tensorflow:global_step/sec: 379.902\n",
      "INFO:tensorflow:loss = 0.30023402, step = 669 (0.263 sec)\n",
      "INFO:tensorflow:global_step/sec: 379.078\n",
      "INFO:tensorflow:loss = 0.348538, step = 769 (0.264 sec)\n",
      "INFO:tensorflow:global_step/sec: 374.44\n",
      "INFO:tensorflow:loss = 0.25698572, step = 869 (0.267 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 938 into model5/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.06445829.\n",
      "Time Take per Iternation of training is : 172.764140%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-02-03:43:03\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-938\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-02-03:43:03\n",
      "INFO:tensorflow:Saving dict for global step 938: accuracy = 0.9658, global_step = 938, loss = 0.11219656\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 938: model5/model.ckpt-938\n",
      "Classification accuracy: 96.58%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-938\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 938 into model5/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.2576459, step = 938\n",
      "INFO:tensorflow:global_step/sec: 316.74\n",
      "INFO:tensorflow:loss = 0.25779805, step = 1038 (0.316 sec)\n",
      "INFO:tensorflow:global_step/sec: 373.762\n",
      "INFO:tensorflow:loss = 0.28813183, step = 1138 (0.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 380.792\n",
      "INFO:tensorflow:loss = 0.1265562, step = 1238 (0.263 sec)\n",
      "INFO:tensorflow:global_step/sec: 371.868\n",
      "INFO:tensorflow:loss = 0.13824713, step = 1338 (0.269 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1407 into model5/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.10329956.\n",
      "Time Take per Iternation of training is : 169.591520%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-02-03:43:05\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-1407\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-02-03:43:05\n",
      "INFO:tensorflow:Saving dict for global step 1407: accuracy = 0.9716, global_step = 1407, loss = 0.08896444\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1407: model5/model.ckpt-1407\n",
      "Classification accuracy: 97.16%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-1407\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1407 into model5/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.13771337, step = 1407\n",
      "INFO:tensorflow:global_step/sec: 315.526\n",
      "INFO:tensorflow:loss = 0.081706926, step = 1507 (0.318 sec)\n",
      "INFO:tensorflow:global_step/sec: 372.263\n",
      "INFO:tensorflow:loss = 0.17883769, step = 1607 (0.268 sec)\n",
      "INFO:tensorflow:global_step/sec: 381.019\n",
      "INFO:tensorflow:loss = 0.09289442, step = 1707 (0.263 sec)\n",
      "INFO:tensorflow:global_step/sec: 377.789\n",
      "INFO:tensorflow:loss = 0.14461821, step = 1807 (0.265 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1876 into model5/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.09986261.\n",
      "Time Take per Iternation of training is : 169.434126%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-02-03:43:07\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-1876\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-02-03:43:07\n",
      "INFO:tensorflow:Saving dict for global step 1876: accuracy = 0.9768, global_step = 1876, loss = 0.0751281\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1876: model5/model.ckpt-1876\n",
      "Classification accuracy: 97.68%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-1876\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1876 into model5/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.28845602, step = 1876\n",
      "INFO:tensorflow:global_step/sec: 311.885\n",
      "INFO:tensorflow:loss = 0.10760119, step = 1976 (0.321 sec)\n",
      "INFO:tensorflow:global_step/sec: 375.129\n",
      "INFO:tensorflow:loss = 0.15122715, step = 2076 (0.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 370.752\n",
      "INFO:tensorflow:loss = 0.1554659, step = 2176 (0.270 sec)\n",
      "INFO:tensorflow:global_step/sec: 369.854\n",
      "INFO:tensorflow:loss = 0.17545813, step = 2276 (0.270 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2345 into model5/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.012598124.\n",
      "Time Take per Iternation of training is : 174.824165%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-02-03:43:09\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-2345\n",
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-02-03:43:09\n",
      "INFO:tensorflow:Saving dict for global step 2345: accuracy = 0.9778, global_step = 2345, loss = 0.06938309\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2345: model5/model.ckpt-2345\n",
      "Classification accuracy: 97.78%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-2345\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2345 into model5/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.1492, step = 2345\n",
      "INFO:tensorflow:global_step/sec: 315.92\n",
      "INFO:tensorflow:loss = 0.09595597, step = 2445 (0.317 sec)\n",
      "INFO:tensorflow:global_step/sec: 366.127\n",
      "INFO:tensorflow:loss = 0.06088163, step = 2545 (0.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 367.534\n",
      "INFO:tensorflow:loss = 0.15092662, step = 2645 (0.272 sec)\n",
      "INFO:tensorflow:global_step/sec: 373.236\n",
      "INFO:tensorflow:loss = 0.13335052, step = 2745 (0.268 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2814 into model5/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.010652813.\n",
      "Time Take per Iternation of training is : 173.178632%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-02-03:43:11\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-2814\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-02-03:43:11\n",
      "INFO:tensorflow:Saving dict for global step 2814: accuracy = 0.9805, global_step = 2814, loss = 0.06177366\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2814: model5/model.ckpt-2814\n",
      "Classification accuracy: 98.05%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-2814\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2814 into model5/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.08336079, step = 2814\n",
      "INFO:tensorflow:global_step/sec: 324.298\n",
      "INFO:tensorflow:loss = 0.06552665, step = 2914 (0.308 sec)\n",
      "INFO:tensorflow:global_step/sec: 375.557\n",
      "INFO:tensorflow:loss = 0.11233252, step = 3014 (0.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 378.135\n",
      "INFO:tensorflow:loss = 0.093229935, step = 3114 (0.264 sec)\n",
      "INFO:tensorflow:global_step/sec: 369.936\n",
      "INFO:tensorflow:loss = 0.096960485, step = 3214 (0.270 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3283 into model5/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.23084126.\n",
      "Time Take per Iternation of training is : 169.354268%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-02-03:43:13\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-3283\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-02-03:43:13\n",
      "INFO:tensorflow:Saving dict for global step 3283: accuracy = 0.9816, global_step = 3283, loss = 0.059023164\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3283: model5/model.ckpt-3283\n",
      "Classification accuracy: 98.16%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-3283\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3283 into model5/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.13684808, step = 3283\n",
      "INFO:tensorflow:global_step/sec: 317.541\n",
      "INFO:tensorflow:loss = 0.10473007, step = 3383 (0.315 sec)\n",
      "INFO:tensorflow:global_step/sec: 382.167\n",
      "INFO:tensorflow:loss = 0.10161126, step = 3483 (0.262 sec)\n",
      "INFO:tensorflow:global_step/sec: 373.33\n",
      "INFO:tensorflow:loss = 0.095867164, step = 3583 (0.268 sec)\n",
      "INFO:tensorflow:global_step/sec: 375.506\n",
      "INFO:tensorflow:loss = 0.01914777, step = 3683 (0.266 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3752 into model5/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.017450323.\n",
      "Time Take per Iternation of training is : 179.452989%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-02-03:43:15\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-3752\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-02-03:43:15\n",
      "INFO:tensorflow:Saving dict for global step 3752: accuracy = 0.9822, global_step = 3752, loss = 0.054425474\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3752: model5/model.ckpt-3752\n",
      "Classification accuracy: 98.22%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-3752\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3752 into model5/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.12202379, step = 3752\n",
      "INFO:tensorflow:global_step/sec: 323.426\n",
      "INFO:tensorflow:loss = 0.066947274, step = 3852 (0.310 sec)\n",
      "INFO:tensorflow:global_step/sec: 383.286\n",
      "INFO:tensorflow:loss = 0.07619253, step = 3952 (0.261 sec)\n",
      "INFO:tensorflow:global_step/sec: 377.508\n",
      "INFO:tensorflow:loss = 0.052446242, step = 4052 (0.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 382.756\n",
      "INFO:tensorflow:loss = 0.13475332, step = 4152 (0.261 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4221 into model5/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.03331224.\n",
      "Time Take per Iternation of training is : 166.505259%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-02-03:43:17\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-4221\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-02-03:43:17\n",
      "INFO:tensorflow:Saving dict for global step 4221: accuracy = 0.9836, global_step = 4221, loss = 0.051277336\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4221: model5/model.ckpt-4221\n",
      "Classification accuracy: 98.36%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-4221\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 4221 into model5/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.1313164, step = 4221\n",
      "INFO:tensorflow:global_step/sec: 309.614\n",
      "INFO:tensorflow:loss = 0.027709747, step = 4321 (0.324 sec)\n",
      "INFO:tensorflow:global_step/sec: 374.411\n",
      "INFO:tensorflow:loss = 0.052379552, step = 4421 (0.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 372.501\n",
      "INFO:tensorflow:loss = 0.16995293, step = 4521 (0.268 sec)\n",
      "INFO:tensorflow:global_step/sec: 379.308\n",
      "INFO:tensorflow:loss = 0.1622725, step = 4621 (0.264 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4690 into model5/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.062961094.\n",
      "Time Take per Iternation of training is : 171.813241%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-02-03:43:19\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-4690\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-02-03:43:19\n",
      "INFO:tensorflow:Saving dict for global step 4690: accuracy = 0.9845, global_step = 4690, loss = 0.050034747\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4690: model5/model.ckpt-4690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy: 98.45%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-4690\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 4690 into model5/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.08215601, step = 4690\n",
      "INFO:tensorflow:global_step/sec: 315.173\n",
      "INFO:tensorflow:loss = 0.12164475, step = 4790 (0.318 sec)\n",
      "INFO:tensorflow:global_step/sec: 362.172\n",
      "INFO:tensorflow:loss = 0.090551384, step = 4890 (0.276 sec)\n",
      "INFO:tensorflow:global_step/sec: 375.674\n",
      "INFO:tensorflow:loss = 0.14107943, step = 4990 (0.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 348.107\n",
      "INFO:tensorflow:loss = 0.03544096, step = 5090 (0.287 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5159 into model5/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.029059693.\n",
      "Time Take per Iternation of training is : 178.628396%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-02-03:43:21\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-5159\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-02-03:43:22\n",
      "INFO:tensorflow:Saving dict for global step 5159: accuracy = 0.9844, global_step = 5159, loss = 0.046733126\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5159: model5/model.ckpt-5159\n",
      "Classification accuracy: 98.44%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-5159\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 5159 into model5/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.037551228, step = 5159\n",
      "INFO:tensorflow:global_step/sec: 312.809\n",
      "INFO:tensorflow:loss = 0.025074294, step = 5259 (0.319 sec)\n",
      "INFO:tensorflow:global_step/sec: 374.716\n",
      "INFO:tensorflow:loss = 0.043564796, step = 5359 (0.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 372.744\n",
      "INFO:tensorflow:loss = 0.04300842, step = 5459 (0.268 sec)\n",
      "INFO:tensorflow:global_step/sec: 370.452\n",
      "INFO:tensorflow:loss = 0.06750565, step = 5559 (0.270 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5628 into model5/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.04929168.\n",
      "Time Take per Iternation of training is : 172.868311%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-02-03:43:23\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-5628\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-02-03:43:24\n",
      "INFO:tensorflow:Saving dict for global step 5628: accuracy = 0.9847, global_step = 5628, loss = 0.047385197\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5628: model5/model.ckpt-5628\n",
      "Classification accuracy: 98.47%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-5628\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 5628 into model5/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.024072845, step = 5628\n",
      "INFO:tensorflow:global_step/sec: 320.271\n",
      "INFO:tensorflow:loss = 0.054997377, step = 5728 (0.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 383.238\n",
      "INFO:tensorflow:loss = 0.06294055, step = 5828 (0.261 sec)\n",
      "INFO:tensorflow:global_step/sec: 377.617\n",
      "INFO:tensorflow:loss = 0.018123444, step = 5928 (0.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 376.711\n",
      "INFO:tensorflow:loss = 0.04426098, step = 6028 (0.266 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6097 into model5/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.09086303.\n",
      "Time Take per Iternation of training is : 170.563448%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-02-03:43:25\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-6097\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-02-03:43:26\n",
      "INFO:tensorflow:Saving dict for global step 6097: accuracy = 0.986, global_step = 6097, loss = 0.044804823\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6097: model5/model.ckpt-6097\n",
      "Classification accuracy: 98.60%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-6097\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 6097 into model5/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.025452074, step = 6097\n",
      "INFO:tensorflow:global_step/sec: 317.09\n",
      "INFO:tensorflow:loss = 0.024302304, step = 6197 (0.316 sec)\n",
      "INFO:tensorflow:global_step/sec: 379.463\n",
      "INFO:tensorflow:loss = 0.026774712, step = 6297 (0.264 sec)\n",
      "INFO:tensorflow:global_step/sec: 375.061\n",
      "INFO:tensorflow:loss = 0.039129693, step = 6397 (0.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 381.82\n",
      "INFO:tensorflow:loss = 0.030518208, step = 6497 (0.262 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6566 into model5/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.046745878.\n",
      "Time Take per Iternation of training is : 177.625908%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-02-03:43:28\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-6566\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-02-03:43:28\n",
      "INFO:tensorflow:Saving dict for global step 6566: accuracy = 0.9868, global_step = 6566, loss = 0.042142723\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6566: model5/model.ckpt-6566\n",
      "Classification accuracy: 98.68%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-6566\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 6566 into model5/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.030607432, step = 6566\n",
      "INFO:tensorflow:global_step/sec: 323.887\n",
      "INFO:tensorflow:loss = 0.009056946, step = 6666 (0.309 sec)\n",
      "INFO:tensorflow:global_step/sec: 372.72\n",
      "INFO:tensorflow:loss = 0.07382008, step = 6766 (0.268 sec)\n",
      "INFO:tensorflow:global_step/sec: 366.377\n",
      "INFO:tensorflow:loss = 0.02982174, step = 6866 (0.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 378.934\n",
      "INFO:tensorflow:loss = 0.067146696, step = 6966 (0.264 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7035 into model5/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.038627386.\n",
      "Time Take per Iternation of training is : 172.981176%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-02-03:43:30\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-7035\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-02-03:43:30\n",
      "INFO:tensorflow:Saving dict for global step 7035: accuracy = 0.9866, global_step = 7035, loss = 0.040226694\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 7035: model5/model.ckpt-7035\n",
      "Classification accuracy: 98.66%"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "model = tf.estimator.Estimator(model_fn=cnn_model_fn,\n",
    "                               params={\"learning_rate\": 1e-4},\n",
    "                               model_dir=\"model5\")\n",
    "\n",
    "import timeit\n",
    "\n",
    "EPOCHS = 15\n",
    "STEP_SIZE = 500\n",
    "count = 0\n",
    "\n",
    "while (count < EPOCHS):\n",
    "    start_time = timeit.default_timer()\n",
    "    model.train(input_fn=train_input_fn, steps=STEP_SIZE)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    sys.stdout.write(\"Time Take per Iternation of training is : {0:%}\".format(elapsed))\n",
    "\n",
    "    result = model.evaluate(input_fn=val_input_fn)\n",
    "    #print(result)\n",
    "    sys.stdout.write(\"Classification accuracy: {0:.2%}\".format(result[\"accuracy\"]))\n",
    "    sys.stdout.flush()\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from model5/model.ckpt-7035\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./trained_models/export/temp-b'1546401550'/saved_model.pbtxt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'./trained_models/export/1546401550'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_serving_input_receiver_fn():\n",
    "    inputs = {'image': tf.placeholder(shape=[28,28,1], dtype=tf.float32, name='image')}\n",
    "    return tf.estimator.export.build_raw_serving_input_receiver_fn(inputs)\n",
    "\n",
    "export_dir = os.path.join('./trained_models/', 'export')\n",
    "\n",
    "if tf.gfile.Exists(export_dir):\n",
    "        tf.gfile.DeleteRecursively(export_dir)\n",
    "\n",
    "model.export_savedmodel(\n",
    "    export_dir_base=export_dir,\n",
    "    serving_input_receiver_fn=make_serving_input_receiver_fn(),\n",
    "    as_text=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./trained_models/export/1546401550/variables/variables\n"
     ]
    }
   ],
   "source": [
    "export_dir = os.path.join('./trained_models/', 'export')\n",
    "saved_model_dir = os.path.join(export_dir, os.listdir(export_dir)[-1]) \n",
    "\n",
    "predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "  export_dir = saved_model_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles.For reference, the tensor object was Tensor(\"Reshape_3:0\", shape=(28, 28, 1), dtype=float32) which was passed to the feed with key Tensor(\"image_1:0\", shape=(?, 28, 1), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-55c3154c00c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"train_data.shape: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/12-23-1/lib/python3.6/site-packages/tensorflow/contrib/predictor/predictor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/12-23-1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/12-23-1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1049\u001b[0m                             \u001b[0;34m'For reference, the tensor object was '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' which was passed to the '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m                             'feed with key ' + str(feed) + '.')\n\u001b[0m\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m           \u001b[0msubfeed_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles.For reference, the tensor object was Tensor(\"Reshape_3:0\", shape=(28, 28, 1), dtype=float32) which was passed to the feed with key Tensor(\"image_1:0\", shape=(?, 28, 1), dtype=float32)."
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "#data_dict = _read_pickle_from_file('cifar-10/cifar-10-batches-py/test_batch')\n",
    "\n",
    "pngFname = target + \"/PNGs/\" + \"train_%d.png\" % (8602)\n",
    "pngFile=open(pngFname,'rb')\n",
    "png_string = pngFile.read();\n",
    "pngFile.close()\n",
    "\n",
    "#sample = tf.cast(tf.image.decode_png(png_string, channels=1), tf.int8)\n",
    "sample = tf.cast(tf.image.decode_png(png_string, channels=1), tf.float32)\n",
    "\n",
    "img = tf.reshape(sample, [28, 28, 1])\n",
    "\n",
    "output = predictor_fn({'image': img})\n",
    "\n",
    "print (\"train_data.shape: \" + str(sample.get_shape()))\n",
    "print (\"train_data.shape: \" + str(img.get_shape()))\n",
    "\n",
    "accuracy = numpy.sum(\n",
    "  [ans==ret for ans, ret in zip(labels, output['classes'])]) / float(N)\n",
    "\n",
    "print(accuracy)\n",
    "\n",
    "#img = sample.reshape([28, 28, 1])\n",
    "\n",
    "\n",
    "N = 1000\n",
    "images = data_dict['data'][:N].reshape([N, 3, 32, 32]).transpose([0, 2, 3, 1])\n",
    "labels = data_dict['labels'][:N]\n",
    "\n",
    "output = predictor_fn({'images': images})\n",
    "accuracy = numpy.sum(\n",
    "  [ans==ret for ans, ret in zip(labels, output['classes'])]) / float(N)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
