{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 01: Writing first MNIST Program\n",
    "By Rahul GAWAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This program is inspired by [GitHub](https://github.com/Hvass-Labs/TensorFlow-Tutorials)\n",
    "\n",
    "Python used: 3.6\n",
    "TensorFlow version: 1.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import urllib.request\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_w = 28\n",
    "img_h = 28\n",
    "img_shape = (img_w, img_h)\n",
    "img_shape_storage = (img_w, img_h, 1)\n",
    "\n",
    "url = \"https://storage.googleapis.com/cvdf-datasets/mnist/\"\n",
    "\n",
    "x_train_file = \"train-images-idx3-ubyte.gz\"\n",
    "y_train_file = \"train-labels-idx1-ubyte.gz\"\n",
    "x_test_file = \"t10k-images-idx3-ubyte.gz\"\n",
    "y_test_file = \"t10k-labels-idx1-ubyte.gz\"\n",
    "\n",
    "target = \"mnist_dataset\"\n",
    "\n",
    "x_train_url = url + x_train_file\n",
    "y_train_url = url + y_train_file\n",
    "x_test_url = url + x_test_file\n",
    "y_test_url = url + y_test_file\n",
    "\n",
    "x_train_rpath = target + \"/\" + x_train_file\n",
    "y_train_rpath = target + \"/\" + y_train_file\n",
    "x_test_rpath = target + \"/\" + x_test_file\n",
    "y_test_rpath = target + \"/\" + y_test_file\n",
    "\n",
    "training_recname = '%s/mnist_%s.tfrecord' % (target, x_train_file)\n",
    "testing_recname = '%s/mnist_%s.tfrecord' % (target, x_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset to a local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print_download_progress(count, block_size, total_size):\n",
    "    \"\"\"\n",
    "    Function used for printing the download progress.\n",
    "    Used as a call-back function in maybe_download_and_extract().\n",
    "    \"\"\"\n",
    "\n",
    "    # Percentage completion.\n",
    "    pct_complete = float(count * block_size) / total_size\n",
    "\n",
    "    # Limit it because rounding errors may cause it to exceed 100%.\n",
    "    pct_complete = min(1.0, pct_complete)\n",
    "\n",
    "    # Status-message. Note the \\r which means the line should overwrite itself.\n",
    "    msg = \"\\r- Download progress: {0:.1%}\".format(pct_complete)\n",
    "\n",
    "    # Print it.\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_img(base_url, filename, download_dir, offset):\n",
    "\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "\n",
    "    file_path = os.path.join(download_dir, filename)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"\\nDownloading\", base_url, \"at\", download_dir, \".\")\n",
    "        file_path, _ = urllib.request.urlretrieve( base_url, file_path, _print_download_progress)\n",
    "\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=offset)\n",
    "\n",
    "    img_size = 28\n",
    "    img_size_flat = img_size * img_size\n",
    "    num_channels = 1\n",
    "    img_shape_full = (img_size, img_size, num_channels)\n",
    "    images_flat = data.reshape(-1, img_size_flat)\n",
    "    return images_flat\n",
    "\n",
    "def download_cls(base_url, filename, download_dir,  offset):\n",
    "\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "\n",
    "    file_path = os.path.join(download_dir, filename)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"\\nDownloading\", base_url, \"at\", download_dir, \".\")\n",
    "        file_path, _ = urllib.request.urlretrieve( base_url, file_path, _print_download_progress)\n",
    "\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8,  offset=offset)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x_train Set 60000\n",
      "y_train_cls Set 60000\n",
      "x_test Set 10000\n",
      "y_test_cls Set 10000\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "\n",
    "#save_path = os.path.join(download_dir, filename)\n",
    "\n",
    "x_train = download_img(x_train_url, x_train_file, target,16)\n",
    "print(\"x_train Set\", len(x_train))\n",
    "\n",
    "y_train_cls = download_cls(y_train_url, y_train_file, target,8)\n",
    "print(\"y_train_cls Set\", len(y_train_cls))\n",
    "\n",
    "x_test = download_img(x_test_url, x_test_file, target,16)\n",
    "print(\"x_test Set\", len(x_test))\n",
    "\n",
    "y_test_cls = download_cls(y_test_url, y_test_file, target,8)\n",
    "print(\"y_test_cls Set\", len(y_test_cls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set 55000  images and  55000  classes.\n",
      "Validation Set 5000  images and  5000  classes.\n",
      "Test Set 10000  images and  10000  classes.\n"
     ]
    }
   ],
   "source": [
    "num_train = 55000\n",
    "num_val = 5000\n",
    "num_test = 10000\n",
    "\n",
    "X_train = x_train[0:num_train] / 255.0\n",
    "Y_train_cls = y_train_cls[0:num_train]\n",
    "\n",
    "X_val = x_train[num_train:] / 255.0\n",
    "Y_val_cls = y_train_cls[num_train:]\n",
    "\n",
    "print(\"Training Set\", len(X_train), \" images and \", len(Y_train_cls), \" classes.\")\n",
    "print(\"Validation Set\", len(X_val), \" images and \", len(Y_val_cls), \" classes.\")\n",
    "\n",
    "X_test = x_test[0:num_test] / 255.0\n",
    "Y_test_cls = y_test_cls[0:num_test]\n",
    "\n",
    "print(\"Test Set\", len(X_test), \" images and \", len(Y_test_cls), \" classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None, img_w=28, img_h=28):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "\n",
    "\n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the first images from the test-set.\n",
    "images = X_test[0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = Y_test_cls[0:9]\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images=images, cls_true=cls_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_cls_char = Y_train_cls.astype(np.int)\n",
    "y_val_cls_char = Y_val_cls.astype(np.int)\n",
    "y_test_cls_char = Y_test_cls.astype(np.int)\n",
    "\n",
    "Y_train_OHE = np.eye(10, dtype=float)[y_train_cls_char]\n",
    "Y_val_OHE = np.eye(10, dtype=float)[y_val_cls_char]\n",
    "Y_test_OHE = np.eye(10, dtype=float)[y_test_cls_char]\n",
    "\n",
    "Y_test_OHE[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-record (Optional)\n",
    "\n",
    "[Referred this implementation to create tf.record](https://github.com/tensorflow/models/blob/master/research/slim/datasets/download_and_convert_mnist.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int64_feature(values):\n",
    "  if not isinstance(values, (tuple, list)):\n",
    "    values = [values]\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tfexample(image_data, image_format, height, width, class_id):\n",
    "  return tf.train.Example(features=tf.train.Features(feature={\n",
    "      'image/encoded':  tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_data])),\n",
    "      'image/format':  tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_format])),\n",
    "      'image/class/label': int64_feature(class_id),\n",
    "      'image/height': int64_feature(height),\n",
    "      'image/width': int64_feature(width)\n",
    "  }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _add_to_tfrecord(data_filename, labels_filename, num_images, tfrecord_writer, images, labels):\n",
    "    with tf.Graph().as_default():\n",
    "        image = tf.placeholder(dtype=tf.uint8, shape=img_shape_storage)\n",
    "        encoded_png = tf.image.encode_png(image)\n",
    "\n",
    "        num_images = len(images)\n",
    "        images = images.reshape(-1, img_w, img_h, 1)\n",
    "\n",
    "        with tf.Session('') as sess:\n",
    "            for j in range(num_images):\n",
    "                sys.stdout.write('\\r>> Converting image %d/%d' % (j + 1, num_images))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                png_string = sess.run(encoded_png, feed_dict={image: images[j]})\n",
    "                example = image_to_tfexample(png_string, 'png'.encode(), img_w, img_h, labels[j])\n",
    "                tfrecord_writer.write(example.SerializeToString())\n",
    "\n",
    "#                pngFname = target + \"/\" + \"train_%d.png\" % (j)\n",
    "#                pngFile=open(pngFname,'wb')\n",
    "#                pngFile.write(png_string);\n",
    "#                pngFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train Set 60000\n",
      "y_train_cls Set 60000\n",
      "x_test Set 10000\n",
      "y_test_cls Set 10000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "x_train = download_img(x_train_url, x_train_file, target,16)\n",
    "print(\"x_train Set\", len(x_train))\n",
    "\n",
    "y_train_cls = download_cls(y_train_url, y_train_file, target,8)\n",
    "print(\"y_train_cls Set\", len(y_train_cls))\n",
    "\n",
    "x_test = download_img(x_test_url, x_test_file, target,16)\n",
    "print(\"x_test Set\", len(x_test))\n",
    "\n",
    "y_test_cls = download_cls(y_test_url, y_test_file, target,8)\n",
    "print(\"y_test_cls Set\", len(y_test_cls))\n",
    "\n",
    "print(len(x_train))\n",
    "\n",
    "if not os.path.exists(training_recname):\n",
    "    with tf.python_io.TFRecordWriter(training_recname) as tfrecord_writer:\n",
    "        _add_to_tfrecord(x_train_rpath, y_train_rpath, 60000, tfrecord_writer, x_train, y_train_cls)\n",
    "\n",
    "if not os.path.exists(testing_recname):\n",
    "    with tf.python_io.TFRecordWriter(testing_recname) as tfrecord_writer:\n",
    "        _add_to_tfrecord(x_test_rpath, y_test_rpath, 10000, tfrecord_writer, x_test, y_test_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(record):\n",
    "    keys_to_features = {\n",
    "        \"image/encoded\":  tf.FixedLenFeature([], tf.string),\n",
    "        \"image/format\":  tf.FixedLenFeature([], tf.string),\n",
    "        \"image/class/label\": tf.FixedLenFeature([], tf.int64),\n",
    "        \"image/height\": tf.FixedLenFeature([], tf.int64),\n",
    "        \"image/width\": tf.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "\n",
    "    parsed = tf.parse_single_example(record, keys_to_features)\n",
    "\n",
    "    height = tf.cast(parsed[\"image/height\"], tf.int32)\n",
    "    width = tf.cast(parsed[\"image/width\"], tf.int32)\n",
    "    label = tf.cast(parsed[\"image/class/label\"], tf.int32)\n",
    "    image = tf.cast(tf.image.decode_png(parsed[\"image/encoded\"], channels=1), tf.float32)\n",
    "    \n",
    " \n",
    "    return {'image': image}, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(filenames):\n",
    "    \n",
    "    BATCH_SIZE = 128\n",
    "    THREADS = 4\n",
    "    PREFETCH = 64\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(filenames=filenames, num_parallel_reads=THREADS)\n",
    "    dataset = dataset.apply(\n",
    "        tf.contrib.data.shuffle_and_repeat(1024, 1)\n",
    "    )\n",
    "    dataset = dataset.apply(\n",
    "        tf.contrib.data.map_and_batch(parser, BATCH_SIZE)\n",
    "    )\n",
    "\n",
    "    dataset = dataset.prefetch(buffer_size=PREFETCH)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    return input_fn(filenames=[training_recname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_input_fn():\n",
    "    return input_fn(filenames=[testing_recname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode, params):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer\n",
    "    # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "    # MNIST images are 28x28 pixels, and have one color channel\n",
    "\n",
    "    net = features[\"image\"]\n",
    "\n",
    "    net = tf.identity(net, name=\"input_tensor\")\n",
    "\n",
    "    net = tf.reshape(net, [-1, 28, 28, 1])\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "    # Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    conv1 = tf.layers.conv2d(inputs=net, filters=32,kernel_size=[5, 5],padding=\"same\", activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    # First max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Convolutional Layer #2\n",
    "    # Computes 64 features using a 5x5 filter.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    conv2 = tf.layers.conv2d(inputs=pool1,filters=64,kernel_size=[5, 5],padding=\"same\",activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #2\n",
    "    # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Flatten tensor into a batch of vectors\n",
    "    # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "    # Dense Layer\n",
    "    # Densely connected layer with 1024 neurons\n",
    "    # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    # Output Tensor Shape: [batch_size, 1024]\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "    # Add dropout operation; 0.6 probability that element will be kept\n",
    "    dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits layer\n",
    "    # Input Tensor Shape: [batch_size, 1024]\n",
    "    # Output Tensor Shape: [batch_size, 10]\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1), \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(loss=loss,global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])\n",
    "    }\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '01_simple_MNIST', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4bc19d9160>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 36.95637, step = 0\n",
      "INFO:tensorflow:global_step/sec: 271.682\n",
      "INFO:tensorflow:loss = 0.6362726, step = 100 (0.367 sec)\n",
      "INFO:tensorflow:global_step/sec: 346.747\n",
      "INFO:tensorflow:loss = 0.39239857, step = 200 (0.288 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.151\n",
      "INFO:tensorflow:loss = 0.35088027, step = 300 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 341.459\n",
      "INFO:tensorflow:loss = 0.2548466, step = 400 (0.293 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 469 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.17922795.\n",
      "Time Take per Iternation of training is : 1320.695740%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:15:51\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-469\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:15:52\n",
      "INFO:tensorflow:Saving dict for global step 469: accuracy = 0.9557, global_step = 469, loss = 0.1387817\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 469: 01_simple_MNIST/model.ckpt-469\n",
      "Classification accuracy: 95.57%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-469\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 469 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.29052535, step = 469\n",
      "INFO:tensorflow:global_step/sec: 296.472\n",
      "INFO:tensorflow:loss = 0.18079144, step = 569 (0.338 sec)\n",
      "INFO:tensorflow:global_step/sec: 346.566\n",
      "INFO:tensorflow:loss = 0.16500208, step = 669 (0.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 349.268\n",
      "INFO:tensorflow:loss = 0.2570778, step = 769 (0.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 348.864\n",
      "INFO:tensorflow:loss = 0.18330508, step = 869 (0.287 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 938 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.07799719.\n",
      "Time Take per Iternation of training is : 191.086901%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:15:54\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-938\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:15:54\n",
      "INFO:tensorflow:Saving dict for global step 938: accuracy = 0.9701, global_step = 938, loss = 0.09487028\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 938: 01_simple_MNIST/model.ckpt-938\n",
      "Classification accuracy: 97.01%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-938\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 938 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.1403925, step = 938\n",
      "INFO:tensorflow:global_step/sec: 297.194\n",
      "INFO:tensorflow:loss = 0.145623, step = 1038 (0.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 350.132\n",
      "INFO:tensorflow:loss = 0.09892328, step = 1138 (0.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 346.802\n",
      "INFO:tensorflow:loss = 0.11691719, step = 1238 (0.288 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.846\n",
      "INFO:tensorflow:loss = 0.13529661, step = 1338 (0.290 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1407 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.20356031.\n",
      "Time Take per Iternation of training is : 183.036698%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:15:56\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-1407\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:15:56\n",
      "INFO:tensorflow:Saving dict for global step 1407: accuracy = 0.9737, global_step = 1407, loss = 0.0783881\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1407: 01_simple_MNIST/model.ckpt-1407\n",
      "Classification accuracy: 97.37%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-1407\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1407 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.13159297, step = 1407\n",
      "INFO:tensorflow:global_step/sec: 298.276\n",
      "INFO:tensorflow:loss = 0.08143024, step = 1507 (0.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 348.945\n",
      "INFO:tensorflow:loss = 0.073127925, step = 1607 (0.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 349.546\n",
      "INFO:tensorflow:loss = 0.19687736, step = 1707 (0.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 350.612\n",
      "INFO:tensorflow:loss = 0.092033625, step = 1807 (0.285 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1876 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.046975482.\n",
      "Time Take per Iternation of training is : 180.450731%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:15:58\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-1876\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:15:58\n",
      "INFO:tensorflow:Saving dict for global step 1876: accuracy = 0.978, global_step = 1876, loss = 0.06634729\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1876: 01_simple_MNIST/model.ckpt-1876\n",
      "Classification accuracy: 97.80%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-1876\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1876 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.08357821, step = 1876\n",
      "INFO:tensorflow:global_step/sec: 294.345\n",
      "INFO:tensorflow:loss = 0.09877599, step = 1976 (0.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 348.185\n",
      "INFO:tensorflow:loss = 0.15935946, step = 2076 (0.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 346.496\n",
      "INFO:tensorflow:loss = 0.12232497, step = 2176 (0.288 sec)\n",
      "INFO:tensorflow:global_step/sec: 346.71\n",
      "INFO:tensorflow:loss = 0.058808073, step = 2276 (0.289 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2345 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.03000605.\n",
      "Time Take per Iternation of training is : 189.187155%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:16:00\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-2345\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:16:01\n",
      "INFO:tensorflow:Saving dict for global step 2345: accuracy = 0.9809, global_step = 2345, loss = 0.05849391\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2345: 01_simple_MNIST/model.ckpt-2345\n",
      "Classification accuracy: 98.09%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-2345\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2345 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.19316745, step = 2345\n",
      "INFO:tensorflow:global_step/sec: 297.066\n",
      "INFO:tensorflow:loss = 0.07530519, step = 2445 (0.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.203\n",
      "INFO:tensorflow:loss = 0.11471896, step = 2545 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 346.236\n",
      "INFO:tensorflow:loss = 0.087194756, step = 2645 (0.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.587\n",
      "INFO:tensorflow:loss = 0.09046361, step = 2745 (0.290 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2814 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.048561085.\n",
      "Time Take per Iternation of training is : 181.752235%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:16:02\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-2814\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:16:03\n",
      "INFO:tensorflow:Saving dict for global step 2814: accuracy = 0.9821, global_step = 2814, loss = 0.0529345\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2814: 01_simple_MNIST/model.ckpt-2814\n",
      "Classification accuracy: 98.21%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-2814\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2814 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.15711571, step = 2814\n",
      "INFO:tensorflow:global_step/sec: 292.093\n",
      "INFO:tensorflow:loss = 0.028838344, step = 2914 (0.343 sec)\n",
      "INFO:tensorflow:global_step/sec: 346.091\n",
      "INFO:tensorflow:loss = 0.12886883, step = 3014 (0.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 340.103\n",
      "INFO:tensorflow:loss = 0.07372536, step = 3114 (0.294 sec)\n",
      "INFO:tensorflow:global_step/sec: 349.078\n",
      "INFO:tensorflow:loss = 0.03476406, step = 3214 (0.286 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3283 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.09855462.\n",
      "Time Take per Iternation of training is : 184.846386%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:16:05\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-3283\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:16:05\n",
      "INFO:tensorflow:Saving dict for global step 3283: accuracy = 0.9832, global_step = 3283, loss = 0.049617775\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3283: 01_simple_MNIST/model.ckpt-3283\n",
      "Classification accuracy: 98.32%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-3283\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3283 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.08104197, step = 3283\n",
      "INFO:tensorflow:global_step/sec: 298.429\n",
      "INFO:tensorflow:loss = 0.04161439, step = 3383 (0.336 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.34\n",
      "INFO:tensorflow:loss = 0.10538702, step = 3483 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 342.242\n",
      "INFO:tensorflow:loss = 0.09643329, step = 3583 (0.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.228\n",
      "INFO:tensorflow:loss = 0.073495746, step = 3683 (0.291 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3752 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.01406768.\n",
      "Time Take per Iternation of training is : 190.055282%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:16:07\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-3752\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:16:07\n",
      "INFO:tensorflow:Saving dict for global step 3752: accuracy = 0.9834, global_step = 3752, loss = 0.046885677\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3752: 01_simple_MNIST/model.ckpt-3752\n",
      "Classification accuracy: 98.34%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-3752\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3752 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.032750208, step = 3752\n",
      "INFO:tensorflow:global_step/sec: 296.069\n",
      "INFO:tensorflow:loss = 0.031820666, step = 3852 (0.338 sec)\n",
      "INFO:tensorflow:global_step/sec: 341.772\n",
      "INFO:tensorflow:loss = 0.09407747, step = 3952 (0.293 sec)\n",
      "INFO:tensorflow:global_step/sec: 346.436\n",
      "INFO:tensorflow:loss = 0.10037069, step = 4052 (0.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 348.721\n",
      "INFO:tensorflow:loss = 0.045951348, step = 4152 (0.287 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4221 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.24771881.\n",
      "Time Take per Iternation of training is : 183.895845%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:16:09\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-4221\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:16:09\n",
      "INFO:tensorflow:Saving dict for global step 4221: accuracy = 0.9839, global_step = 4221, loss = 0.043826986\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4221: 01_simple_MNIST/model.ckpt-4221\n",
      "Classification accuracy: 98.39%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-4221\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 4221 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.09540823, step = 4221\n",
      "INFO:tensorflow:global_step/sec: 289.391\n",
      "INFO:tensorflow:loss = 0.0992838, step = 4321 (0.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.08\n",
      "INFO:tensorflow:loss = 0.15241921, step = 4421 (0.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 346.186\n",
      "INFO:tensorflow:loss = 0.10525054, step = 4521 (0.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 340.326\n",
      "INFO:tensorflow:loss = 0.06618193, step = 4621 (0.294 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4690 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0075335153.\n",
      "Time Take per Iternation of training is : 189.247688%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:16:11\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-4690\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:16:12\n",
      "INFO:tensorflow:Saving dict for global step 4690: accuracy = 0.9856, global_step = 4690, loss = 0.042594187\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4690: 01_simple_MNIST/model.ckpt-4690\n",
      "Classification accuracy: 98.56%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-4690\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 4690 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.035251632, step = 4690\n",
      "INFO:tensorflow:global_step/sec: 294.2\n",
      "INFO:tensorflow:loss = 0.023004089, step = 4790 (0.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.915\n",
      "INFO:tensorflow:loss = 0.046674583, step = 4890 (0.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 345.3\n",
      "INFO:tensorflow:loss = 0.09657391, step = 4990 (0.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 349.703\n",
      "INFO:tensorflow:loss = 0.11483623, step = 5090 (0.286 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5159 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.051236823.\n",
      "Time Take per Iternation of training is : 197.738175%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:16:14\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-5159\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:16:14\n",
      "INFO:tensorflow:Saving dict for global step 5159: accuracy = 0.9862, global_step = 5159, loss = 0.040143024\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5159: 01_simple_MNIST/model.ckpt-5159\n",
      "Classification accuracy: 98.62%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-5159\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 5159 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.08839243, step = 5159\n",
      "INFO:tensorflow:global_step/sec: 293.668\n",
      "INFO:tensorflow:loss = 0.071945995, step = 5259 (0.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.113\n",
      "INFO:tensorflow:loss = 0.09915255, step = 5359 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.829\n",
      "INFO:tensorflow:loss = 0.033088908, step = 5459 (0.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.935\n",
      "INFO:tensorflow:loss = 0.055922426, step = 5559 (0.290 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5628 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.03608651.\n",
      "Time Take per Iternation of training is : 182.384432%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:16:16\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-5628\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:16:16\n",
      "INFO:tensorflow:Saving dict for global step 5628: accuracy = 0.9862, global_step = 5628, loss = 0.03939421\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5628: 01_simple_MNIST/model.ckpt-5628\n",
      "Classification accuracy: 98.62%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-5628\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 5628 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.09491094, step = 5628\n",
      "INFO:tensorflow:global_step/sec: 293.153\n",
      "INFO:tensorflow:loss = 0.049020305, step = 5728 (0.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 346.928\n",
      "INFO:tensorflow:loss = 0.045482986, step = 5828 (0.288 sec)\n",
      "INFO:tensorflow:global_step/sec: 347.633\n",
      "INFO:tensorflow:loss = 0.050506208, step = 5928 (0.288 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.628\n",
      "INFO:tensorflow:loss = 0.081267074, step = 6028 (0.290 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6097 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.029680291.\n",
      "Time Take per Iternation of training is : 186.318027%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:16:18\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-6097\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:16:18\n",
      "INFO:tensorflow:Saving dict for global step 6097: accuracy = 0.9866, global_step = 6097, loss = 0.038486585\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6097: 01_simple_MNIST/model.ckpt-6097\n",
      "Classification accuracy: 98.66%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-6097\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 6097 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.04532703, step = 6097\n",
      "INFO:tensorflow:global_step/sec: 294.979\n",
      "INFO:tensorflow:loss = 0.038077682, step = 6197 (0.339 sec)\n",
      "INFO:tensorflow:global_step/sec: 345.734\n",
      "INFO:tensorflow:loss = 0.082544625, step = 6297 (0.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 348.188\n",
      "INFO:tensorflow:loss = 0.031048073, step = 6397 (0.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.951\n",
      "INFO:tensorflow:loss = 0.01826299, step = 6497 (0.291 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6566 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00388046.\n",
      "Time Take per Iternation of training is : 190.791524%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:16:20\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-6566\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:16:21\n",
      "INFO:tensorflow:Saving dict for global step 6566: accuracy = 0.987, global_step = 6566, loss = 0.036968276\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6566: 01_simple_MNIST/model.ckpt-6566\n",
      "Classification accuracy: 98.70%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-6566\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 6566 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.04702773, step = 6566\n",
      "INFO:tensorflow:global_step/sec: 292.75\n",
      "INFO:tensorflow:loss = 0.08696365, step = 6666 (0.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 345.625\n",
      "INFO:tensorflow:loss = 0.05156362, step = 6766 (0.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 349.247\n",
      "INFO:tensorflow:loss = 0.06491026, step = 6866 (0.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.679\n",
      "INFO:tensorflow:loss = 0.052295025, step = 6966 (0.290 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7035 into 01_simple_MNIST/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.041454986.\n",
      "Time Take per Iternation of training is : 190.108539%INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2019-01-07-23:16:23\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-7035\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-07-23:16:23\n",
      "INFO:tensorflow:Saving dict for global step 7035: accuracy = 0.9882, global_step = 7035, loss = 0.037539206\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 7035: 01_simple_MNIST/model.ckpt-7035\n",
      "Classification accuracy: 98.82%"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "model = tf.estimator.Estimator(model_fn=cnn_model_fn,\n",
    "                               params={\"learning_rate\": 1e-4},\n",
    "                               model_dir=\"01_simple_MNIST\")\n",
    "\n",
    "import timeit\n",
    "\n",
    "EPOCHS = 15\n",
    "STEP_SIZE = 500\n",
    "count = 0\n",
    "\n",
    "while (count < EPOCHS):\n",
    "    start_time = timeit.default_timer()\n",
    "    model.train(input_fn=train_input_fn, steps=STEP_SIZE)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    sys.stdout.write(\"Time Take per Iternation of training is : {0:%}\".format(elapsed))\n",
    "\n",
    "    result = model.evaluate(input_fn=val_input_fn)\n",
    "    #print(result)\n",
    "    sys.stdout.write(\"Classification accuracy: {0:.2%}\".format(result[\"accuracy\"]))\n",
    "    sys.stdout.flush()\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from 01_simple_MNIST/model.ckpt-7035\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./01_simple_MNIST/export/temp-b'1546902983'/saved_model.pbtxt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'./01_simple_MNIST/export/1546902983'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_serving_input_receiver_fn():\n",
    "    inputs = {'image': tf.placeholder(shape=[28,28,1], dtype=tf.float32, name='image')}\n",
    "    return tf.estimator.export.build_raw_serving_input_receiver_fn(inputs)\n",
    "\n",
    "export_dir = os.path.join('./01_simple_MNIST/', 'export')\n",
    "\n",
    "if tf.gfile.Exists(export_dir):\n",
    "        tf.gfile.DeleteRecursively(export_dir)\n",
    "\n",
    "model.export_savedmodel(\n",
    "    export_dir_base=export_dir,\n",
    "    serving_input_receiver_fn=make_serving_input_receiver_fn(),\n",
    "    as_text=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./01_simple_MNIST/export/1546902983/variables/variables\n"
     ]
    }
   ],
   "source": [
    "export_dir = os.path.join('./01_simple_MNIST/', 'export')\n",
    "saved_model_dir = os.path.join(export_dir, os.listdir(export_dir)[-1]) \n",
    "\n",
    "predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "  export_dir = saved_model_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Result: Pred. No. : 2')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy\n",
    "import random\n",
    "\n",
    "dir = target + \"/PNGs/\"\n",
    "pngFname = dir + random.choice(os.listdir(dir))\n",
    "\n",
    "pngFile=open(pngFname,'rb')\n",
    "png_string = pngFile.read();\n",
    "pngFile.close()\n",
    "\n",
    "#sample = tf.cast(tf.image.decode_png(png_string, channels=1), tf.int8)\n",
    "sample = tf.cast(tf.image.decode_png(png_string, channels=1), tf.float32)\n",
    "\n",
    "a = np.ones(shape=(28,28,1),dtype=np.float32)    \n",
    "\n",
    "img = tf.reshape(sample, [28, 28, 1])\n",
    "\n",
    "with sess.as_default():\n",
    "    a = img.eval()\n",
    "\n",
    "output = predictor_fn({'image': a})\n",
    "#print(output)\n",
    "#print (\"train_data.shape: \" + str(sample.get_shape()))\n",
    "#print (\"train_data.shape: \" + str(img.get_shape()))\n",
    "\n",
    "max = 0\n",
    "\n",
    "for x in range(10):\n",
    "    if output['probabilities'][0][x] >= output['probabilities'][0][max]:\n",
    "        max = x\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.imshow(a.reshape(img_shape), cmap='binary')\n",
    "xlabel = \"Result: Pred. No. : {0}\".format(max)\n",
    "ax.set_title(xlabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
